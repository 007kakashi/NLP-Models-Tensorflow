{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'<pad>': 0}\n",
    "tag2idx = {'<pad>': 0}\n",
    "word_idx = 1\n",
    "tag_idx = 1\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for line in open('pos_train.txt'):\n",
    "    line = line.rstrip()\n",
    "    if line:\n",
    "        word, tag, _ = line.split()\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = word_idx\n",
    "            word_idx += 1\n",
    "        x_train.append(word2idx[word])\n",
    "        if tag not in tag2idx:\n",
    "            tag2idx[tag] = tag_idx\n",
    "            tag_idx += 1\n",
    "        y_train.append(tag2idx[tag])\n",
    "        \n",
    "word2idx['<unknown>'] = word_idx\n",
    "\n",
    "for line in open('pos_test.txt'):\n",
    "    line = line.rstrip()\n",
    "    if line:\n",
    "        word, tag, _ = line.split()\n",
    "        if word in word2idx:\n",
    "            x_test.append(word2idx[word])\n",
    "        else:\n",
    "            x_test.append(word_idx)\n",
    "        y_test.append(tag2idx[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'seq_len': 20,\n",
    "    'batch_size': 128,\n",
    "    'hidden_dim': 128,\n",
    "    'clip_norm': 5.0,\n",
    "    'text_iter_step': 1,\n",
    "    'lr': {'start': 5e-3, 'end': 5e-4},\n",
    "    'n_epoch': 1,\n",
    "    'display_step': 50,\n",
    "    'vocab_size': len(word2idx),\n",
    "    'n_class':tag_idx,\n",
    "    'kernel_sizes': [3, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_seq(x):\n",
    "    return np.array([x[i: i+params['seq_len']] for i in range(0, len(x)-params['seq_len'], params['text_iter_step'])])\n",
    "\n",
    "def to_train_seq(*args):\n",
    "    return [iter_seq(x) for x in args]\n",
    "\n",
    "def to_test_seq(*args):\n",
    "    return [np.reshape(x[:(len(x)-len(x)%params['seq_len'])],\n",
    "        [-1,params['seq_len']]) for x in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_train_seq(x_train, y_train)\n",
    "X_test, Y_test = to_test_seq(x_test, y_test)\n",
    "params['lr']['steps'] = len(X_train) // params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.X = tf.placeholder(tf.int32, [None, params['seq_len']])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, params['seq_len']])\n",
    "        \n",
    "        def rnn_cell():\n",
    "            return tf.nn.rnn_cell.GRUCell(params['hidden_dim'],\n",
    "                                          kernel_initializer=tf.orthogonal_initializer())\n",
    "        \n",
    "        def clip_grads(loss):\n",
    "            variables = tf.trainable_variables()\n",
    "            grads = tf.gradients(loss, variables)\n",
    "            clipped_grads, _ = tf.clip_by_global_norm(grads, params['clip_norm'])\n",
    "            return zip(clipped_grads, variables)\n",
    "        \n",
    "        self.embedding = tf.Variable(tf.truncated_normal([params['vocab_size'], params['hidden_dim']],\n",
    "                                                      stddev=1.0 / np.sqrt(params['hidden_dim'])))\n",
    "        embedded = tf.nn.embedding_lookup(self.embedding, self.X)\n",
    "        embedded = tf.nn.dropout(embedded,0.1)\n",
    "        \n",
    "        pad = tf.zeros([tf.shape(embedded)[0], 1, params['hidden_dim']])\n",
    "        for kernel in params['kernel_sizes']:\n",
    "            n = (kernel - 1) // 2\n",
    "            _embedded = tf.concat([pad]*n + [embedded] + [pad]*n, 1)\n",
    "            embedded += tf.layers.conv1d(_embedded, params['hidden_dim'], kernel, activation=tf.nn.relu)\n",
    "        \n",
    "        self.logits = tf.layers.dense(embedded, params['n_class'])\n",
    "        log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "        self.logits, self.Y, tf.count_nonzero(self.X, 1))\n",
    "        self.cost = tf.reduce_mean(-log_likelihood)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(params['lr']['start'],\n",
    "                                                        self.global_step, params['lr']['steps'],\n",
    "                                                        params['lr']['end']/params['lr']['start'])\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(clip_grads(self.cost), \n",
    "                                                                                    global_step=self.global_step)\n",
    "        self.crf_decode = tf.contrib.crf.crf_decode(self.logits, \n",
    "                                                    trans_params, \n",
    "                                                    tf.count_nonzero(self.X, 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 1, loss 79.439484\n",
      "epoch 1, step 50, loss 25.554974\n",
      "epoch 1, step 100, loss 18.444744\n",
      "epoch 1, step 150, loss 15.207127\n",
      "epoch 1, step 200, loss 17.540133\n",
      "epoch 1, step 250, loss 10.940320\n",
      "epoch 1, step 300, loss 10.688293\n",
      "epoch 1, step 350, loss 13.418294\n",
      "epoch 1, step 400, loss 10.662128\n",
      "epoch 1, step 450, loss 9.231688\n",
      "epoch 1, step 500, loss 10.172064\n",
      "epoch 1, step 550, loss 8.602699\n",
      "epoch 1, step 600, loss 10.297527\n",
      "epoch 1, step 650, loss 7.816038\n",
      "epoch 1, step 700, loss 4.592223\n",
      "epoch 1, step 750, loss 4.128824\n",
      "epoch 1, step 800, loss 7.665007\n",
      "epoch 1, step 850, loss 7.643055\n",
      "epoch 1, step 900, loss 7.236738\n",
      "epoch 1, step 950, loss 6.907567\n",
      "epoch 1, step 1000, loss 1.634830\n",
      "epoch 1, step 1050, loss 6.755506\n",
      "epoch 1, step 1100, loss 4.212307\n",
      "epoch 1, step 1150, loss 5.266168\n",
      "epoch 1, step 1200, loss 5.731796\n",
      "epoch 1, step 1250, loss 4.186990\n",
      "epoch 1, step 1300, loss 7.574566\n",
      "epoch 1, step 1350, loss 3.262103\n",
      "epoch 1, step 1400, loss 4.344210\n",
      "epoch 1, step 1450, loss 2.775949\n",
      "epoch 1, step 1500, loss 2.943261\n",
      "epoch 1, step 1550, loss 12.098641\n",
      "epoch 1, step 1600, loss 3.289165\n",
      "epoch 1, step 1650, loss 2.303090\n",
      "epoch 1, avg loss 10.319273\n"
     ]
    }
   ],
   "source": [
    "for i in range(params['n_epoch']):\n",
    "    total_cost = 0\n",
    "    for k in range(0,(X_train.shape[0] // params['batch_size'])*params['batch_size'],params['batch_size']):\n",
    "        batch_x = X_train[k:k+params['batch_size']]\n",
    "        batch_y = Y_train[k:k+params['batch_size']]\n",
    "        step, loss, _ = sess.run([model.global_step, model.cost, model.optimizer],\n",
    "                                feed_dict={model.X:batch_x, model.Y:batch_y})\n",
    "        if step % params['display_step'] == 0 or step == 1:\n",
    "            print('epoch %d, step %d, loss %f'%(i+1,step,loss))\n",
    "        total_cost += loss\n",
    "    total_cost /= ((X_train.shape[0] // params['batch_size']))\n",
    "    print('epoch %d, avg loss %f'%(i+1,total_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        VBZ       0.88      0.89      0.89      6639\n",
      "         ``       0.97      0.98      0.98      5070\n",
      "        JJR       0.99      0.99      0.99      4020\n",
      "         VB       0.88      0.86      0.87       912\n",
      "        PRP       0.90      0.80      0.85      1354\n",
      "        PDT       0.83      0.69      0.76      1103\n",
      "        VBG       0.99      0.98      0.99      1177\n",
      "        JJS       0.91      0.85      0.88      1269\n",
      "        VBD       0.79      0.83      0.81      2962\n",
      "         ''       0.89      0.83      0.86      3034\n",
      "         NN       0.81      0.90      0.85      4803\n",
      "          $       0.99      0.99      0.99      2389\n",
      "        POS       0.99      0.97      0.98      1214\n",
      "          )       0.96      0.95      0.96       433\n",
      "         DT       0.99      0.99      0.99      1974\n",
      "          (       0.86      0.84      0.85       539\n",
      "         UH       0.80      0.67      0.73       727\n",
      "        SYM       0.98      0.95      0.97       421\n",
      "         MD       0.89      0.91      0.90      1918\n",
      "         RB       0.98      0.97      0.97       323\n",
      "         FW       0.99      0.97      0.98       316\n",
      "          .       0.85      0.91      0.88      1679\n",
      "       NNPS       0.93      0.77      0.84        48\n",
      "        NNS       0.97      0.96      0.97       470\n",
      "        WP$       1.00      0.27      0.43        11\n",
      "         RP       0.94      0.79      0.86        77\n",
      "       PRP$       0.98      0.98      0.98       384\n",
      "        WRB       0.90      0.81      0.85        77\n",
      "         TO       0.85      0.17      0.28       130\n",
      "        RBR       0.98      0.96      0.97       814\n",
      "          ,       0.86      0.74      0.80        77\n",
      "         CD       0.95      0.93      0.94       110\n",
      "        VBN       0.75      0.54      0.63        70\n",
      "          :       0.80      0.84      0.82       202\n",
      "         IN       0.92      0.81      0.86       202\n",
      "         WP       0.95      0.96      0.95        93\n",
      "         EX       0.91      0.84      0.87        49\n",
      "         CC       0.86      0.60      0.71        10\n",
      "        WDT       0.67      0.17      0.27        12\n",
      "        RBS       0.97      0.96      0.97       238\n",
      "          #       0.00      0.00      0.00         4\n",
      "         JJ       0.50      0.50      0.50         4\n",
      "        VBP       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.91      0.91      0.90     47360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 43, does not match size of target_names, 45\n",
      "  .format(len(labels), len(target_names))\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = sess.run(model.crf_decode,feed_dict={model.X:X_test})\n",
    "print(classification_report(Y_test.ravel(), Y_pred.ravel(), target_names=tag2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
