{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ctexts.json','r') as fopen:\n",
    "    ctexts = json.loads(fopen.read())[:100]\n",
    "    \n",
    "with open('headlines.json','r') as fopen:\n",
    "    headlines = json.loads(fopen.read())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 6546\n",
      "Most common words [('the', 2378), ('comma', 1806), ('dot', 1774), ('and', 981), ('to', 951), ('of', 910)]\n",
      "Sample data [4, 812, 7, 850, 251, 12, 156, 2434, 10, 549] ['the', 'daman', 'and', 'diu', 'administration', 'on', 'wednesday', 'withdrew', 'a', 'circular']\n"
     ]
    }
   ],
   "source": [
    "concat_from = ' '.join(ctexts).split()\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
    "print('vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab to size: 654\n",
      "Most common words [('to', 34), ('in', 28), ('for', 17), ('s', 14), ('comma', 13), ('of', 13)]\n",
      "Sample data [70, 15, 74, 583, 215, 110, 5, 205, 87, 573] ['daman', 'and', 'diu', 'revokes', 'mandatory', 'rakshabandhan', 'in', 'offices', 'order', 'malaika']\n"
     ]
    }
   ],
   "source": [
    "concat_to = ' '.join(headlines).split()\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
    "print('vocab to size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daman and diu revokes mandatory rakshabandhan in offices order EOS'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(headlines)):\n",
    "    headlines[i] = headlines[i] + ' EOS'\n",
    "headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i.split():\n",
    "            try:\n",
    "                ints.append(dic[k])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                ints.append(UNK)\n",
    "        X.append(ints)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = str_idx(ctexts, dictionary_from)\n",
    "Y = str_idx(headlines, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarization:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size, \n",
    "                 from_dict_size, to_dict_size, batch_size, \n",
    "                 dropout = 0.5, beam_width = 15):\n",
    "        \n",
    "        def lstm_cell(reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, reuse=reuse)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.placeholder(tf.int32, [None])\n",
    "        self.Y_seq_len = tf.placeholder(tf.int32, [None])\n",
    "        # encoder\n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        encoder_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        encoder_dropout = tf.contrib.rnn.DropoutWrapper(encoder_cells, output_keep_prob = 0.5)\n",
    "        self.encoder_out, self.encoder_state = tf.nn.dynamic_rnn(cell = encoder_dropout, \n",
    "                                                                 inputs = encoder_embedded, \n",
    "                                                                 sequence_length = self.X_seq_len,\n",
    "                                                                 dtype = tf.float32)\n",
    "        \n",
    "        self.encoder_state = tuple(self.encoder_state[-1] for _ in range(num_layers))\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        # decoder\n",
    "        decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
    "        decoder_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        dense_layer = tf.layers.Dense(to_dict_size)\n",
    "        training_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input),\n",
    "                sequence_length = self.Y_seq_len,\n",
    "                embedding = decoder_embeddings,\n",
    "                sampling_probability = 0.5,\n",
    "                time_major = False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell = decoder_cells,\n",
    "                helper = training_helper,\n",
    "                initial_state = self.encoder_state,\n",
    "                output_layer = dense_layer)\n",
    "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = training_decoder,\n",
    "                impute_finished = True,\n",
    "                maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
    "        predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                cell = decoder_cells,\n",
    "                embedding = decoder_embeddings,\n",
    "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
    "                end_token = EOS,\n",
    "                initial_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, beam_width),\n",
    "                beam_width = beam_width,\n",
    "                output_layer = dense_layer,\n",
    "                length_penalty_weight = 0.0)\n",
    "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = predicting_decoder,\n",
    "                impute_finished = False,\n",
    "                maximum_iterations = tf.reduce_max(self.X_seq_len)//3)\n",
    "        self.training_logits = training_decoder_output.rnn_output\n",
    "        self.predicting_ids = predicting_decoder_output.predicted_ids[:, :, 0]\n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 512\n",
    "num_layers = 2\n",
    "embedded_size = 128\n",
    "batch_size = 16\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Summarization(size_layer, num_layers, embedded_size, len(dictionary_from), \n",
    "                len(dictionary_to), batch_size)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int, maxlen=500):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = min(max([len(sentence) for sentence in sentence_batch]),maxlen)\n",
    "    for sentence in sentence_batch:\n",
    "        sentence = sentence[:maxlen]\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens\n",
    "\n",
    "def check_accuracy(logits, Y):\n",
    "    acc = 0\n",
    "    for i in range(logits.shape[0]):\n",
    "        internal_acc = 0\n",
    "        count = 0\n",
    "        for k in range(len(Y[i])):\n",
    "            try:\n",
    "                if Y[i][k] == logits[i][k]:\n",
    "                    internal_acc += 1\n",
    "                count += 1\n",
    "                if Y[i][k] == EOS:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "        acc += (internal_acc / count)\n",
    "    return acc / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 6.266043, valid loss 5.895721\n",
      "expected output: twenty stuck in gurugram mall lift for ninety mins rescued\n",
      "predicted output: \n",
      "epoch: 1, avg loss: 6.373687, avg accuracy: 0.001250 \n",
      "\n",
      "epoch 2, train loss 5.680036, valid loss 5.670321\n",
      "expected output: rape accused kills self after order to drink victim s urine\n",
      "predicted output: to to to\n",
      "epoch: 2, avg loss: 5.825993, avg accuracy: 0.041833 \n",
      "\n",
      "epoch 3, train loss 5.522484, valid loss 5.448895\n",
      "expected output: hotel staff to get training to spot signs of sex trafficking\n",
      "predicted output: to to to to to to to\n",
      "epoch: 3, avg loss: 5.609531, avg accuracy: 0.023046 \n",
      "\n",
      "epoch 4, train loss 5.345392, valid loss 5.219334\n",
      "expected output: swara talks about unequal pay for actresses in bollywood\n",
      "predicted output: virgin to to to to in\n",
      "epoch: 4, avg loss: 5.415331, avg accuracy: 0.039509 \n",
      "\n",
      "epoch 5, train loss 5.195951, valid loss 5.059280\n",
      "expected output: thirteen people die in a day while travelling in mumbai locals\n",
      "predicted output: virgin to to in in in\n",
      "epoch: 5, avg loss: 5.248505, avg accuracy: 0.045933 \n",
      "\n",
      "epoch 6, train loss 5.048729, valid loss 4.844576\n",
      "expected output: shops relocated to decongest delhi airport\n",
      "predicted output: virgin to to to to to\n",
      "epoch: 6, avg loss: 5.093947, avg accuracy: 0.061176 \n",
      "\n",
      "epoch 7, train loss 4.905863, valid loss 4.785278\n",
      "expected output: rape accused kills self after order to drink victim s urine\n",
      "predicted output: and comma comma for to to in\n",
      "epoch: 7, avg loss: 4.899732, avg accuracy: 0.072842 \n",
      "\n",
      "epoch 8, train loss 4.807846, valid loss 6.908104\n",
      "expected output: photo shows how close flight came to crashing onto four planes\n",
      "predicted output: tom hiddleston to to to to to in\n",
      "epoch: 8, avg loss: 4.717532, avg accuracy: 0.119957 \n",
      "\n",
      "epoch 9, train loss 4.583410, valid loss 4.333841\n",
      "expected output: thirteen people die in a day while travelling in mumbai locals\n",
      "predicted output: thirteen thirteen thirteen in in in in in\n",
      "epoch: 9, avg loss: 4.525395, avg accuracy: 0.122564 \n",
      "\n",
      "epoch 10, train loss 4.445206, valid loss 4.087462\n",
      "expected output: mob assaults man caught with woman in compromising position\n",
      "predicted output: man man to to in in in\n",
      "epoch: 10, avg loss: 4.294152, avg accuracy: 0.162825 \n",
      "\n",
      "epoch 11, train loss 3.918944, valid loss 3.519608\n",
      "expected output: swara talks about unequal pay for actresses in bollywood\n",
      "predicted output: swara talks unequal in in in\n",
      "epoch: 11, avg loss: 3.934700, avg accuracy: 0.211642 \n",
      "\n",
      "epoch 12, train loss 3.640592, valid loss 3.108156\n",
      "expected output: not suffering from depression comma it s just a big term kapil\n",
      "predicted output: suffering suffering suffering from from to to s s\n",
      "epoch: 12, avg loss: 3.553135, avg accuracy: 0.281871 \n",
      "\n",
      "epoch 13, train loss 3.354476, valid loss 2.731150\n",
      "expected output: shops relocated to decongest delhi airport\n",
      "predicted output: shops relocated relocated to to to to\n",
      "epoch: 13, avg loss: 3.238724, avg accuracy: 0.325000 \n",
      "\n",
      "epoch 14, train loss 3.138373, valid loss 2.839550\n",
      "expected output: victoria and abdul costumes to be displayed at queen s home\n",
      "predicted output: victoria and and abdul be be be\n",
      "epoch: 14, avg loss: 2.945857, avg accuracy: 0.373323 \n",
      "\n",
      "epoch 15, train loss 2.633666, valid loss 3.579501\n",
      "expected output: delhi hc reduces aid for negligent accident victim by 45\n",
      "predicted output: delhi delhi reduces for for for for victim victim\n",
      "epoch: 15, avg loss: 2.610998, avg accuracy: 0.417453 \n",
      "\n",
      "epoch 16, train loss 2.272100, valid loss 2.104221\n",
      "expected output: hc to hear fresh appeals of talwars in aarushi murder case\n",
      "predicted output: hc hc hear hear appeals appeals appeals talwars in\n",
      "epoch: 16, avg loss: 2.289614, avg accuracy: 0.467444 \n",
      "\n",
      "epoch 17, train loss 2.237608, valid loss 1.667650\n",
      "expected output: shops relocated to decongest delhi airport\n",
      "predicted output: shops relocated decongest delhi airport\n",
      "epoch: 17, avg loss: 2.073265, avg accuracy: 0.462268 \n",
      "\n",
      "epoch 18, train loss 1.773014, valid loss 1.654186\n",
      "expected output: 60 yr old lynched over rumours she was cutting people s hair\n",
      "predicted output: 60 yr old lynched rumours rumours she\n",
      "epoch: 18, avg loss: 1.879927, avg accuracy: 0.516048 \n",
      "\n",
      "epoch 19, train loss 1.592593, valid loss 1.508636\n",
      "expected output: malaika slams user who trolled her for divorcing rich man\n",
      "predicted output: malaika slams user user who who for for divorcing\n",
      "epoch: 19, avg loss: 1.624318, avg accuracy: 0.585090 \n",
      "\n",
      "epoch 20, train loss 1.570828, valid loss 1.175105\n",
      "expected output: delhi s aiims comma safdarjung to be declared no hawker zone\n",
      "predicted output: delhi s aiims comma safdarjung to be declared declared hawker\n",
      "epoch: 20, avg loss: 1.483168, avg accuracy: 0.600403 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    for k in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x, seq_x = pad_sentence_batch(train_X[k: k+batch_size], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(train_Y[k: k+batch_size], PAD)\n",
    "        predicted, loss, _ = sess.run([model.predicting_ids, model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y,\n",
    "                                                model.X_seq_len:seq_x,\n",
    "                                                model.Y_seq_len:seq_y})\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_accuracy += check_accuracy(predicted,batch_y)\n",
    "        \n",
    "    rand = np.random.randint(0, len(train_X)-batch_size)\n",
    "    batch_x, seq_x = pad_sentence_batch(train_X[rand:rand+batch_size], PAD)\n",
    "    batch_y, seq_y = pad_sentence_batch(train_Y[rand:rand+batch_size], PAD)\n",
    "    predicted, test_loss = sess.run([model.predicting_ids,model.cost], \n",
    "                                    feed_dict={model.X:batch_x,\n",
    "                                               model.Y:batch_y,\n",
    "                                               model.X_seq_len:seq_x,\n",
    "                                               model.Y_seq_len:seq_y})\n",
    "    print('epoch %d, train loss %f, valid loss %f'%(i+1,loss,test_loss))\n",
    "    print('expected output:',' '.join([rev_dictionary_to[n] for n in batch_y[0] if n not in [-1,0,1,2,3]]))\n",
    "    print('predicted output:',' '.join([rev_dictionary_to[n] for n in predicted[0] if n not in [-1,0,1,2,3]]))\n",
    "            \n",
    "    total_loss /= (len(train_X) // batch_size)\n",
    "    total_accuracy /= (len(train_X) // batch_size)\n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
