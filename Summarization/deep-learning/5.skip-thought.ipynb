{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,w2v,maxlen=50, \n",
    "                 vocabulary_size=20000,\n",
    "                 output_size=512, \n",
    "                 learning_rate=1e-3,\n",
    "                 embedding_size = 256,\n",
    "                 batch_size=16,\n",
    "                 max_grad_norm=10):\n",
    "        special_embeddings = tf.get_variable(\n",
    "            'special_embeddings',\n",
    "            shape=[4, embedding_size],\n",
    "            initializer=tf.initializers.random_uniform(-np.sqrt(3), np.sqrt(3)),\n",
    "            trainable=False)\n",
    "        word_embeddings = tf.get_variable(\n",
    "            \"word_embeddings\", \n",
    "            shape=[vocabulary_size, embedding_size],\n",
    "            initializer=tf.initializers.constant(w2v.vectors[:vocabulary_size]),\n",
    "            trainable=False)\n",
    "        self.global_step = tf.get_variable(\n",
    "            \"global_step\", shape=[], trainable=False,\n",
    "            initializer=tf.initializers.zeros())\n",
    "        self.embeddings = tf.concat([special_embeddings, word_embeddings], 0)\n",
    "        self.output_layer = tf.layers.Dense(vocabulary_size, name=\"output_layer\")\n",
    "        self.output_layer.build(output_size)\n",
    "        \n",
    "        self.BEFORE = tf.placeholder(tf.int32,[None,maxlen])\n",
    "        self.INPUT = tf.placeholder(tf.int32,[None,maxlen])\n",
    "        self.AFTER = tf.placeholder(tf.int32,[None,maxlen])\n",
    "        \n",
    "        self.get_thought = self.thought(self.INPUT)\n",
    "        fw_logits = self.decoder(self.get_thought, self.AFTER)\n",
    "        bw_logits = self.decoder(self.get_thought, self.BEFORE)\n",
    "        self.loss = self.calculate_loss(fw_logits, self.AFTER) + self.calculate_loss(bw_logits, self.BEFORE)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), max_grad_norm)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).apply_gradients(\n",
    "            zip(grads, tvars), global_step=self.global_step)\n",
    "        \n",
    "    def get_embedding(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "        \n",
    "    def thought(self, inputs):\n",
    "        encoder_in = self.get_embedding(inputs)\n",
    "        fw_cell = tf.nn.rnn_cell.GRUCell(output_size)\n",
    "        bw_cell = tf.nn.rnn_cell.GRUCell(output_size)\n",
    "        sequence_length = tf.reduce_sum(tf.sign(inputs), axis=1)\n",
    "        rnn_output = tf.nn.bidirectional_dynamic_rnn(\n",
    "            fw_cell, bw_cell, encoder_in, sequence_length=sequence_length,\n",
    "            dtype=tf.float32)[1]\n",
    "        return sum(rnn_output)\n",
    "        \n",
    "    def decoder(self, thought, labels):\n",
    "        sos_tokens = tf.constant([[2]] * batch_size, dtype=tf.int32)\n",
    "        shifted_labels = tf.concat([sos_tokens, labels[:,:-1]], 1)\n",
    "        decoder_in = self.get_embedding(shifted_labels)\n",
    "        cell = tf.nn.rnn_cell.GRUCell(output_size)\n",
    "        max_seq_lengths = tf.constant([maxlen] * batch_size)\n",
    "        helper = seq2seq.TrainingHelper(decoder_in, max_seq_lengths, time_major=False)\n",
    "        decoder = seq2seq.BasicDecoder(cell, helper, thought)\n",
    "        decoder_out = seq2seq.dynamic_decode(decoder)[0].rnn_output\n",
    "        return decoder_out\n",
    "        \n",
    "    def calculate_loss(self, outputs, labels):\n",
    "        mask = tf.cast(tf.sign(labels), tf.float32)\n",
    "        logits = self.output_layer(outputs)\n",
    "        return seq2seq.sequence_loss(logits, labels, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences(s):\n",
    "    result = []\n",
    "    for sentence in s.split('.'):\n",
    "        sentence = re.sub(r\"[^A-Za-z0-9 ']\", \" \", sentence)\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence).strip()\n",
    "        result.append(sentence)\n",
    "    return result\n",
    "\n",
    "def sequence(s, w2v_model, maxlen, vocabulary_size):\n",
    "    words = s.split()\n",
    "    np_array = np.zeros((maxlen),dtype=np.int32)\n",
    "    current_no = 0\n",
    "    for no, word in enumerate(words[:maxlen - 2]):\n",
    "        id_to_append = 1\n",
    "        if word in w2v_model:\n",
    "            word_id = w2v_model.vocab[word].index + 4\n",
    "            if word_id < vocabulary_size:\n",
    "                id_to_append = word_id\n",
    "        np_array[no] = id_to_append\n",
    "        current_no = no\n",
    "    np_array[current_no + 1] = 3\n",
    "    return np_array\n",
    "\n",
    "def generate_batch(sentences,batch_size,w2v_model,maxlen,vocabulary_size):\n",
    "    window_size = batch_size + 2\n",
    "    first_index = random.randint(0, len(sentences) - window_size)\n",
    "    batch_sentences = sentences[first_index:first_index+window_size]\n",
    "    batch_sequences = np.array([sequence(sentence,w2v_model,maxlen,vocabulary_size) for sentence in batch_sentences])\n",
    "    window_shape = []\n",
    "    for i in range(batch_size):\n",
    "        window_shape.append(batch_sequences[i:i+3])\n",
    "    window_shape = np.array(window_shape)\n",
    "    return window_shape[:,0], window_shape[:,1], window_shape[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 28.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "contents = []\n",
    "for filename in tqdm(os.listdir('books')):\n",
    "    with open(os.path.join('books', filename)) as f:\n",
    "        contents.extend(sentences(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.vectors[:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "vocabulary_size = 20000\n",
    "output_size = 300\n",
    "learning_rate = 1e-3\n",
    "embedding_size = w2v_model.vectors[:1].shape[1]\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(w2v_model,embedding_size=embedding_size,output_size=output_size)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5000/5000 [11:44<00:00,  7.42it/s, cost=8.77] \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [11:44<00:00,  7.06it/s, cost=8.69]    \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [11:45<00:00,  6.88it/s, cost=7.81]    \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [11:47<00:00,  6.75it/s, cost=9.93]    \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [11:43<00:00,  6.83it/s, cost=7.54]    \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [11:56<00:00,  6.45it/s, cost=6.9]     \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [12:05<00:00,  7.09it/s, cost=8.92]    \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [12:09<00:00,  6.54it/s, cost=7.74]    \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [12:01<00:00,  7.24it/s, cost=7.79]    \n",
      "train minibatch loop: 100%|██████████| 5000/5000 [11:51<00:00,  6.98it/s, cost=8.4]     \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pbar = tqdm(range(0, 5000), desc='train minibatch loop')\n",
    "    for p in pbar:\n",
    "        bw_input, current_input, fw_input = generate_batch(contents,batch_size,w2v_model,maxlen,vocabulary_size)\n",
    "        loss, _ = sess.run([model.loss, model.optimizer], \n",
    "                           feed_dict = {model.BEFORE : bw_input, \n",
    "                                        model.INPUT : current_input,\n",
    "                                        model.AFTER: fw_input})\n",
    "        pbar.set_postfix(cost=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('books/Blood_Born') as f:\n",
    "    book = sentences(f.read())\n",
    "\n",
    "book_sequences = [sequence(sentence, w2v_model, maxlen, vocabulary_size) for sentence in book]\n",
    "encoded = sess.run(model.get_thought,feed_dict={model.INPUT:np.array(book_sequences)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was no way Scarlett would stay that her family would allow such things. Victoria was still unable to wrap her head around how he had survived in the first place. But it had to be done. Heard it. Either Scarlett had intentionally slipped him which wasn t at all a far fetched idea or she had been found and taken. She knew how to use it if she needed it. This book may not be re sold or given away to other people. Someone has to and apparently no one in this country has the balls to do it. It was a warning shot telling Brodie to stay the fuck back if he knew what was good for him. It would dissolve before she even had a chance to see it. But he had never been one to not poke at others. It would be long and torturous. He had to find Scarlett. Or starting too. It would be long and torturous. The only way she was getting out of there was if they stopped drugging her and she could blast the door open or if someone let her out. He shrugged. Now it was barely there. She was seething but below that anger Hurt. He just needed to focus on that. But it would keep her power at bay until the witch Gwendoline pulled it from her. So be it. It had been how he got close to Scarlett in the first place how he had obtained a position beneath the King. Brodie snapped knowing there was nothing he could say to get through to her but trying anyway. Forcing him to blink hands rising to cover his eyes as he groaned turning to his side and sitting up. No. wordpress. Scarlett dragged herself into a sitting position eyes on the woman before her. Brodie took a few steps back body flush with the front of hers as he pushed them out of the way. Or at least most of it. Chapter Eleven Brodie remained in the bathroom for a bit after Scarlett left. I like ya Scarlett. Just shut up and take a seat lass. and actually be in any way aroused. Her hand moved off his back moving up to his hair as she tangled her fingers in it. But it would keep her power at bay until the witch Gwendoline pulled it from her. She did body begging for him as she wiggled against him pressing his hard length against her. just. With a sigh she pushed off the railing and started after him. Violently if Ronan had his way. I do hate it when ye do that. She knew how to use it if she needed it. And Brodie. She actually felt bad for him. . He had enough. Matt sighed hand going to the other gun on his left side. Two. You ve got it. So. so much like him. Maybe yer right. We try not to meddle in the affairs of those below but. Damn it all. Fine. I hope so. Come sit with me. You re supposed to have my back when we go in there. Always one to tread carefully despite her knee jerk past. get her lips back on his throat Scarlett swallowed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "n_clusters = int(np.ceil(len(encoded)**0.5))\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans = kmeans.fit(encoded)\n",
    "avg = []\n",
    "closest = []\n",
    "for j in range(n_clusters):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,encoded)\n",
    "ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "print('. '.join([book[closest[idx]] for idx in ordering]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
