{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya\n",
    "import re\n",
    "from malaya.texts._text_functions import split_into_sentences\n",
    "from malaya.texts import _regex\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "tokenizer = malaya.preprocessing._tokenizer\n",
    "splitter = split_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def preprocessing(w):\n",
    "    if is_number_regex(w):\n",
    "        return '<NUM>'\n",
    "    elif re.match(_regex._money, w):\n",
    "        return '<MONEY>'\n",
    "    elif re.match(_regex._date, w):\n",
    "        return '<DATE>'\n",
    "    elif re.match(_regex._expressions['email'], w):\n",
    "        return '<EMAIL>'\n",
    "    elif re.match(_regex._expressions['url'], w):\n",
    "        return '<URL>'\n",
    "    else:\n",
    "        w = ''.join(''.join(s)[:2] for _, s in itertools.groupby(w))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'PAD': 0,\n",
       "  'UNK': 1,\n",
       "  '_ROOT': 2,\n",
       "  '<NUM>': 3,\n",
       "  '<MONEY>': 4,\n",
       "  '<DATE>': 5,\n",
       "  '<URL>': 6,\n",
       "  '<EMAIL>': 7},\n",
       " {'PAD': 0,\n",
       "  'UNK': 1,\n",
       "  '_ROOT': 2,\n",
       "  '<NUM>': 3,\n",
       "  '<MONEY>': 4,\n",
       "  '<DATE>': 5,\n",
       "  '<URL>': 6,\n",
       "  '<EMAIL>': 7})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {'PAD': 0,'UNK':1, '_ROOT': 2}\n",
    "tag2idx = {'PAD': 0, '_<ROOT>': 1}\n",
    "char2idx = {'PAD': 0,'UNK':1, '_ROOT': 2}\n",
    "word_idx = 3\n",
    "tag_idx = 2\n",
    "char_idx = 3\n",
    "\n",
    "special_tokens = ['<NUM>', '<MONEY>', '<DATE>', '<URL>', '<EMAIL>']\n",
    "\n",
    "for t in special_tokens:\n",
    "    word2idx[t] = word_idx\n",
    "    word_idx += 1\n",
    "    char2idx[t] = char_idx\n",
    "    char_idx += 1\n",
    "    \n",
    "word2idx, char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"_PAD\"\n",
    "PAD_POS = \"_PAD_POS\"\n",
    "PAD_TYPE = \"_<PAD>\"\n",
    "PAD_CHAR = \"_PAD_CHAR\"\n",
    "ROOT = \"_ROOT\"\n",
    "ROOT_POS = \"_ROOT_POS\"\n",
    "ROOT_TYPE = \"_<ROOT>\"\n",
    "ROOT_CHAR = \"_ROOT_CHAR\"\n",
    "END = \"_END\"\n",
    "END_POS = \"_END_POS\"\n",
    "END_TYPE = \"_<END>\"\n",
    "END_CHAR = \"_END_CHAR\"\n",
    "\n",
    "def process_corpus(corpus, until = None):\n",
    "    global word2idx, tag2idx, char2idx, word_idx, tag_idx, char_idx\n",
    "    sentences, words, depends, labels, pos, chars = [], [], [], [], [], []\n",
    "    temp_sentence, temp_word, temp_depend, temp_label, temp_pos = [], [], [], [], []\n",
    "    first_time = True\n",
    "    for sentence in corpus:\n",
    "        try:\n",
    "            if len(sentence):\n",
    "                if sentence[0] == '#':\n",
    "                    continue\n",
    "                if first_time:\n",
    "                    print(sentence)\n",
    "                    first_time = False\n",
    "                sentence = sentence.split('\\t')\n",
    "                for c in sentence[1]:\n",
    "                    if c not in char2idx:\n",
    "                        char2idx[c] = char_idx\n",
    "                        char_idx += 1\n",
    "                if sentence[7] not in tag2idx:\n",
    "                    tag2idx[sentence[7]] = tag_idx\n",
    "                    tag_idx += 1\n",
    "                sentence[1] = preprocessing(sentence[1])\n",
    "                if sentence[1] not in word2idx:\n",
    "                    word2idx[sentence[1]] = word_idx\n",
    "                    word_idx += 1\n",
    "                temp_word.append(word2idx[sentence[1]])\n",
    "                temp_depend.append(int(sentence[6]))\n",
    "                temp_label.append(tag2idx[sentence[7]])\n",
    "                temp_sentence.append(sentence[1])\n",
    "                temp_pos.append(sentence[3])\n",
    "            else:\n",
    "                if len(temp_sentence) < 2:\n",
    "                    temp_word = []\n",
    "                    temp_depend = []\n",
    "                    temp_label = []\n",
    "                    temp_sentence = []\n",
    "                    temp_pos = []\n",
    "                    continue\n",
    "                words.append([word2idx['_ROOT']] + temp_word)\n",
    "                depends.append([0] + temp_depend)\n",
    "                labels.append([tag2idx['_<ROOT>']] + temp_label)\n",
    "                sentences.append([ROOT] + temp_sentence)\n",
    "                pos.append([ROOT_POS] + temp_pos)\n",
    "                char_ = [[char2idx['_ROOT']]]\n",
    "                for w in temp_sentence:\n",
    "                    if w in char2idx:\n",
    "                        char_.append([char2idx[w]])\n",
    "                    else:\n",
    "                        char_.append([char2idx[c] for c in w])\n",
    "                chars.append(char_)\n",
    "                temp_word = []\n",
    "                temp_depend = []\n",
    "                temp_label = []\n",
    "                temp_sentence = []\n",
    "                temp_pos = []\n",
    "        except:\n",
    "            print(sentence)\n",
    "    return sentences, words[:-1], depends[:-1], labels[:-1], pos[:-1], chars[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tFrom\tfrom\tADP\tIN\t_\t3\tcase\t3:case\t_\n",
      "['10.1', 'has', 'have', 'VERB', 'VBZ', '_', '_', '_', '8:parataxis', 'CopyOf=-1']\n",
      "['21.1', 'has', 'have', 'VERB', 'VBZ', '_', '_', '_', '16:conj:and', 'CopyOf=-1']\n"
     ]
    }
   ],
   "source": [
    "with open('en_ewt-ud-dev.conllu') as fopen:\n",
    "    corpus = fopen.read().split('\\n')\n",
    "\n",
    "sentences, words, depends, labels, pos, chars = process_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 3, 3, 4, 0, 6, 4, 4],\n",
       " [1, 2, 3, 4, 5, 3, 6, 7],\n",
       " ['_ROOT_POS', 'ADP', 'DET', 'PROPN', 'VERB', 'DET', 'NOUN', 'PUNCT'],\n",
       " [2, 8, 9, 10, 11, 12, 13, 14],\n",
       " ['_ROOT', 'From', 'the', 'AP', 'comes', 'this', 'story', ':'],\n",
       " [[2],\n",
       "  [8, 9, 10, 11],\n",
       "  [12, 13, 14],\n",
       "  [15, 16],\n",
       "  [17, 10, 11, 14, 18],\n",
       "  [12, 13, 19, 18],\n",
       "  [18, 12, 10, 9, 20],\n",
       "  [21]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depends[0], labels[0], pos[0], words[0], sentences[0], chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _obtain_child_index_for_left2right(heads):\n",
    "    child_ids = [[] for _ in range(len(heads))]\n",
    "    # skip the symbolic root.\n",
    "    for child in range(1, len(heads)):\n",
    "        head = heads[child]\n",
    "        child_ids[head].append(child)\n",
    "    return child_ids\n",
    "\n",
    "\n",
    "def _obtain_child_index_for_inside_out(heads):\n",
    "    child_ids = [[] for _ in range(len(heads))]\n",
    "    for head in range(len(heads)):\n",
    "        # first find left children inside-out\n",
    "        for child in reversed(range(1, head)):\n",
    "            if heads[child] == head:\n",
    "                child_ids[head].append(child)\n",
    "        # second find right children inside-out\n",
    "        for child in range(head + 1, len(heads)):\n",
    "            if heads[child] == head:\n",
    "                child_ids[head].append(child)\n",
    "    return child_ids\n",
    "\n",
    "\n",
    "def _obtain_child_index_for_depth(heads, reverse):\n",
    "    def calc_depth(head):\n",
    "        children = child_ids[head]\n",
    "        max_depth = 0\n",
    "        for child in children:\n",
    "            depth = calc_depth(child)\n",
    "            child_with_depth[head].append((child, depth))\n",
    "            max_depth = max(max_depth, depth + 1)\n",
    "        child_with_depth[head] = sorted(child_with_depth[head], key=lambda x: x[1], reverse=reverse)\n",
    "        return max_depth\n",
    "\n",
    "    child_ids = _obtain_child_index_for_left2right(heads)\n",
    "    child_with_depth = [[] for _ in range(len(heads))]\n",
    "    calc_depth(0)\n",
    "    return [[child for child, depth in child_with_depth[head]] for head in range(len(heads))]\n",
    "\n",
    "\n",
    "def _generate_stack_inputs(heads, types, prior_order):\n",
    "    if prior_order == 'deep_first':\n",
    "        child_ids = _obtain_child_index_for_depth(heads, True)\n",
    "    elif prior_order == 'shallow_first':\n",
    "        child_ids = _obtain_child_index_for_depth(heads, False)\n",
    "    elif prior_order == 'left2right':\n",
    "        child_ids = _obtain_child_index_for_left2right(heads)\n",
    "    elif prior_order == 'inside_out':\n",
    "        child_ids = _obtain_child_index_for_inside_out(heads)\n",
    "    else:\n",
    "        raise ValueError('Unknown prior order: %s' % prior_order)\n",
    "\n",
    "    stacked_heads = []\n",
    "    children = []\n",
    "    siblings = []\n",
    "    stacked_types = []\n",
    "    skip_connect = []\n",
    "    prev = [0 for _ in range(len(heads))]\n",
    "    sibs = [0 for _ in range(len(heads))]\n",
    "    stack = [0]\n",
    "    position = 1\n",
    "    while len(stack) > 0:\n",
    "        head = stack[-1]\n",
    "        stacked_heads.append(head)\n",
    "        siblings.append(sibs[head])\n",
    "        child_id = child_ids[head]\n",
    "        skip_connect.append(prev[head])\n",
    "        prev[head] = position\n",
    "        if len(child_id) == 0:\n",
    "            children.append(head)\n",
    "            sibs[head] = 0\n",
    "            stacked_types.append(tag2idx['PAD'])\n",
    "            stack.pop()\n",
    "        else:\n",
    "            child = child_id.pop(0)\n",
    "            children.append(child)\n",
    "            sibs[head] = child\n",
    "            stack.append(child)\n",
    "            stacked_types.append(types[child])\n",
    "        position += 1\n",
    "\n",
    "    return stacked_heads, children, siblings, stacked_types, skip_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1901, 1901, 1901, 1901, 1902, 1901)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(depends), len(labels), len(pos), len(words), len(sentences), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_buckets = [10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100, 140]\n",
    "\n",
    "data = [[] for _ in _buckets]\n",
    "max_char_length = [0 for _ in _buckets]\n",
    "\n",
    "for i in range(len(depends)):\n",
    "    for bucket_id, bucket_size in enumerate(_buckets):\n",
    "        if len(words[i]) < bucket_size:\n",
    "            stacked_heads, children, siblings, stacked_types, skip_connect \\\n",
    "            = _generate_stack_inputs(depends[i], labels[i], 'deep_first')\n",
    "            data[bucket_id].append([words[i], pos[i], depends[i], labels[i], chars[i],\n",
    "                                    stacked_heads, children, siblings, stacked_types, skip_connect])\n",
    "            max_len = max([len(char_seq) for char_seq in chars[i]])\n",
    "            if max_char_length[bucket_id] < max_len:\n",
    "                max_char_length[bucket_id] = max_len\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 4, 3, 1, 3, 2, 3, 4, 6, 5, 6, 4, 7, 4, 0],\n",
       " [4, 3, 1, 1, 2, 2, 3, 6, 5, 5, 6, 7, 7, 4, 0],\n",
       " [0, 0, 0, 0, 1, 0, 2, 3, 0, 0, 5, 6, 0, 7, 4],\n",
       " [5, 4, 2, 0, 3, 0, 0, 6, 3, 0, 0, 7, 0, 0, 0],\n",
       " [0, 0, 0, 0, 3, 0, 5, 2, 0, 0, 9, 8, 0, 12, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_generate_stack_inputs(depends[0], labels[0], 'deep_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 8, 9, 10, 11, 12, 13, 14],\n",
       " ['_ROOT_POS', 'ADP', 'DET', 'PROPN', 'VERB', 'DET', 'NOUN', 'PUNCT'],\n",
       " [0, 3, 3, 4, 0, 6, 4, 4],\n",
       " [1, 2, 3, 4, 5, 3, 6, 7],\n",
       " [[2],\n",
       "  [8, 9, 10, 11],\n",
       "  [12, 13, 14],\n",
       "  [15, 16],\n",
       "  [17, 10, 11, 14, 18],\n",
       "  [12, 13, 19, 18],\n",
       "  [18, 12, 10, 9, 20],\n",
       "  [21]],\n",
       " [0, 4, 3, 1, 3, 2, 3, 4, 6, 5, 6, 4, 7, 4, 0],\n",
       " [4, 3, 1, 1, 2, 2, 3, 6, 5, 5, 6, 7, 7, 4, 0],\n",
       " [0, 0, 0, 0, 1, 0, 2, 3, 0, 0, 5, 6, 0, 7, 4],\n",
       " [5, 4, 2, 0, 3, 0, 0, 6, 3, 0, 0, 7, 0, 0, 0],\n",
       " [0, 0, 0, 0, 3, 0, 5, 2, 0, 0, 9, 8, 0, 12, 1]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [sent.word_ids, sent.char_id_seqs, inst.pos_ids, inst.heads, \n",
    "# inst.type_ids, stacked_heads, children, siblings, stacked_types, skip_connect]\n",
    "bucket_id = 0\n",
    "bucket_sizes = [len(data[b]) for b in range(len(_buckets))]\n",
    "bucket_length = _buckets[0]\n",
    "bucket_size = bucket_sizes[0]\n",
    "bucket_size, bucket_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_length = max_char_length[bucket_id]\n",
    "wid_inputs = np.zeros([bucket_size, bucket_length], dtype=np.int32)\n",
    "cid_inputs = np.zeros([bucket_size, bucket_length, char_length], dtype=np.int32)\n",
    "hid_inputs = np.zeros([bucket_size, bucket_length], dtype=np.int32)\n",
    "tid_inputs = np.zeros([bucket_size, bucket_length], dtype=np.int32)\n",
    "\n",
    "stack_hid_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "chid_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "ssid_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "stack_tid_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "skip_connect_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "\n",
    "masks_e = np.zeros([bucket_size, bucket_length], dtype=np.float32)\n",
    "masks_d = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.float32)\n",
    "lengths_d = np.empty(bucket_size, dtype=np.int32)\n",
    "\n",
    "for i, inst in enumerate(data[bucket_id]):\n",
    "    w, p, d, l, ch, stack_hids, chids, ssids, stack_tids, skip_ids = inst\n",
    "    inst_size = len(w)\n",
    "    wid_inputs[i, :inst_size] = w\n",
    "    for c, cids in enumerate(ch):\n",
    "        cid_inputs[i, c, :len(cids)] = cids\n",
    "    tid_inputs[i, :inst_size] = l\n",
    "    hid_inputs[i, :inst_size] = d\n",
    "    masks_e[i, :inst_size] = 1.0\n",
    "    \n",
    "    inst_size_decoder = 2 * inst_size - 1\n",
    "    stack_hid_inputs[i, :inst_size_decoder] = stack_hids\n",
    "    chid_inputs[i, :inst_size_decoder] = chids\n",
    "    ssid_inputs[i, :inst_size_decoder] = ssids\n",
    "    stack_tid_inputs[i, :inst_size_decoder] = stack_tids\n",
    "    skip_connect_inputs[i, :inst_size_decoder] = skip_ids\n",
    "    masks_d[i, :inst_size_decoder] = 1.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class PriorOrder(Enum):\n",
    "    DEPTH = 0\n",
    "    INSIDE_OUT = 1\n",
    "    LEFT2RIGTH = 2\n",
    "\n",
    "class BiAAttention:\n",
    "    def __init__(self, input_size_encoder, input_size_decoder, num_labels):\n",
    "        self.input_size_encoder = input_size_encoder\n",
    "        self.input_size_decoder = input_size_decoder\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.W_d = tf.get_variable(\"W_d\", shape=[self.num_labels, self.input_size_decoder],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.W_e = tf.get_variable(\"W_e\", shape=[self.num_labels, self.input_size_encoder],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.U = tf.get_variable(\"U\", shape=[self.num_labels, self.input_size_decoder, self.input_size_encoder],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "    def forward(self, input_d, input_e, mask_d=None, mask_e=None):\n",
    "        batch = tf.shape(input_d)[0]\n",
    "        length_decoder = tf.shape(input_d)[1]\n",
    "        length_encoder = tf.shape(input_e)[1]\n",
    "        out_d = tf.expand_dims(tf.matmul(self.W_d, tf.transpose(input_d, [0, 2, 1])), 3)\n",
    "        out_e = tf.expand_dims(tf.matmul(self.W_e, tf.transpose(input_e, [0, 2, 1])), 2)\n",
    "        output = tf.matmul(tf.expand_dims(input_d, 1), self.U)\n",
    "        output = tf.matmul(output, tf.transpose(tf.expand_dims(input_e, 1), [0, 1, 3, 2]))\n",
    "        \n",
    "        output = output + out_d + out_e\n",
    "        \n",
    "        if mask_d is not None:\n",
    "            d = tf.expand_dims(tf.expand_dims(mask_d, 1), 3)\n",
    "            e = tf.expand_dims(tf.expand_dims(mask_e, 1), 2)\n",
    "            output = output * d * e\n",
    "            \n",
    "        return output\n",
    "    \n",
    "class BiLinear:\n",
    "    def __init__(self, left_features, right_features, out_features):\n",
    "        self.left_features = left_features\n",
    "        self.right_features = right_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.U = tf.get_variable(\"U-bi\", shape=[out_features, left_features, right_features],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.W_l = tf.get_variable(\"Wl\", shape=[out_features, left_features],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.W_r = tf.get_variable(\"Wr\", shape=[out_features, right_features],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    def forward(self, input_left, input_right):\n",
    "        left_size = tf.shape(input_left)\n",
    "        output_shape = tf.concat([left_size[:-1], [self.out_features]], axis = 0)\n",
    "        batch = tf.cast(tf.reduce_prod(left_size[:-1]), tf.int32)\n",
    "        input_left = tf.reshape(input_left, (batch, self.left_features))\n",
    "        input_right = tf.reshape(input_right, (batch, self.right_features))\n",
    "        tiled = tf.tile(tf.expand_dims(input_left, axis = 0), (self.out_features,1,1))\n",
    "        output = tf.transpose(tf.reduce_sum(tf.matmul(tiled, self.U), axis = 2))\n",
    "        output = output + tf.matmul(input_left, tf.transpose(self.W_l))\\\n",
    "        + tf.matmul(input_right, tf.transpose(self.W_r))\n",
    "        \n",
    "        return tf.reshape(output, output_shape)\n",
    "\n",
    "class StackPointer:\n",
    "    def __init__(self, word_dim, num_words, char_dim, num_chars, num_filters, kernel_size,\n",
    "                 input_size_decoder, hidden_size, encoder_layers, decoder_layers,\n",
    "                 num_labels, arc_space, type_space):\n",
    "        \n",
    "        def cells(size, reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size,\n",
    "                                           initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
    "        \n",
    "        self.word_embedd = tf.Variable(tf.random_uniform([num_words, word_dim], -1, 1))\n",
    "        self.char_embedd = tf.Variable(tf.random_uniform([num_chars, char_dim], -1, 1))\n",
    "        self.conv1d = tf.layers.Conv1D(num_filters, kernel_size, 1, padding='VALID')\n",
    "        self.num_labels = num_labels\n",
    "        self.prior_order = PriorOrder.DEPTH\n",
    "        self.encoder = tf.nn.rnn_cell.MultiRNNCell([cells(hidden_size) for _ in range(encoder_layers)])\n",
    "        self.decoder = tf.nn.rnn_cell.MultiRNNCell([cells(hidden_size) for _ in range(decoder_layers)])\n",
    "        \n",
    "        self.src_dense = tf.layers.Dense(hidden_size)\n",
    "        self.hx_dense = tf.layers.Dense(hidden_size)\n",
    "\n",
    "        self.arc_h = tf.layers.Dense(arc_space)\n",
    "        self.arc_c = tf.layers.Dense(arc_space)\n",
    "        self.attention = BiAAttention(arc_space, arc_space, 1)\n",
    "\n",
    "        self.type_h = tf.layers.Dense(type_space)\n",
    "        self.type_c = tf.layers.Dense(type_space)\n",
    "        self.bilinear = BiLinear(type_space, type_space, self.num_labels)\n",
    "        \n",
    "    def encode(self, input_word, input_char):\n",
    "        word = tf.nn.embedding_lookup(self.word_embedd, input_word)\n",
    "        char = tf.nn.embedding_lookup(self.char_embedd, input_char)\n",
    "        b = tf.shape(char)[0]\n",
    "        wl = tf.shape(char)[1]\n",
    "        cl = tf.shape(char)[2]\n",
    "        d = char.shape[3]\n",
    "        char = tf.reshape(char, [b * wl, cl, d])\n",
    "        char = tf.reduce_max(self.conv1d(char), axis = 1)\n",
    "        char = tf.nn.tanh(char)\n",
    "        d = char.shape[-1]\n",
    "        char = tf.reshape(char, [b, wl, d])\n",
    "        \n",
    "        src_encoding = tf.concat([word, char], axis=2)\n",
    "        output, hn = tf.nn.dynamic_rnn(self.encoder, src_encoding, dtype = tf.float32,\n",
    "                                      scope = 'encoder')\n",
    "        return output, hn\n",
    "    \n",
    "    def decode(self, output_encoder, heads, heads_stack, siblings, hn):\n",
    "        batch = tf.shape(output_encoder)[0]\n",
    "        batch_index = tf.range(0, batch)\n",
    "        t = tf.transpose(heads_stack)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        src_encoding = tf.gather_nd(output_encoder, concatenated)\n",
    "        \n",
    "        mask_sibs = tf.expand_dims(tf.cast(tf.not_equal(siblings, 0), tf.float32), axis = 2)\n",
    "        t = tf.transpose(siblings)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        output_enc_sibling = tf.gather_nd(output_encoder, concatenated) * mask_sibs\n",
    "        src_encoding = src_encoding + output_enc_sibling\n",
    "        \n",
    "        t = tf.transpose(heads_stack)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                       tf.expand_dims(t, axis = 0)],axis = 0))\n",
    "        g = tf.transpose(tf.gather_nd(heads, concatenated))\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(g))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                               tf.expand_dims(g, axis = 0)],axis = 0))\n",
    "        output_enc_gpar = tf.gather_nd(output_encoder, concatenated)\n",
    "        src_encoding = src_encoding + output_enc_gpar\n",
    "        \n",
    "        src_encoding = tf.nn.elu(self.src_dense(src_encoding))\n",
    "        output, hn = tf.nn.dynamic_rnn(self.decoder, src_encoding, dtype = tf.float32,\n",
    "                                      initial_state = hn,\n",
    "                                      scope = 'decoder')\n",
    "        return output, hn\n",
    "    \n",
    "    def loss(self, input_word, input_char, \n",
    "             heads, stacked_heads, children, siblings, stacked_types,\n",
    "             mask_e, mask_d,\n",
    "             label_smooth = 1.0):\n",
    "        \n",
    "        output_enc, hn_enc = self.encode(input_word, input_char)\n",
    "        arc_c = tf.nn.elu(self.arc_c(output_enc))\n",
    "        type_c = tf.nn.elu(self.type_c(output_enc))\n",
    "        \n",
    "        output_dec, _ = self.decode(output_enc, heads, stacked_heads, siblings, hn_enc)\n",
    "        arc_h = tf.nn.elu(self.arc_h(output_dec))\n",
    "        type_h = tf.nn.elu(self.type_h(output_dec))\n",
    "        \n",
    "        max_len_d = tf.shape(arc_h)[1]\n",
    "        \n",
    "        out_arc = tf.squeeze(self.attention.forward(arc_h, arc_c, mask_d=mask_d, mask_e=mask_e), axis = 1)\n",
    "        batch = tf.shape(arc_c)[0]\n",
    "        max_len_e = tf.shape(arc_c)[1]\n",
    "        batch_index = tf.range(0, batch)\n",
    "        \n",
    "        t = tf.transpose(children)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        type_c = tf.gather_nd(type_c, concatenated)\n",
    "        out_type = self.bilinear.forward(type_h, type_c)\n",
    "        print(out_arc.shape,out_type.shape)\n",
    "        \n",
    "        minus_inf = -1e8\n",
    "        minus_mask_d = (1 - mask_d) * minus_inf\n",
    "        minus_mask_e = (1 - mask_e) * minus_inf\n",
    "        \n",
    "        out_arc = out_arc + tf.expand_dims(minus_mask_d, 2) + tf.expand_dims(minus_mask_e, 1)\n",
    "        loss_arc = tf.nn.log_softmax(out_arc, axis = 2)\n",
    "        loss_type = tf.nn.log_softmax(out_type, axis = 2)\n",
    "        coverage = tf.cumsum(tf.exp(loss_arc), axis = 1)\n",
    "        \n",
    "        mask_leaf = tf.cast(tf.equal(children, stacked_heads), tf.float32)\n",
    "        mask_non_leaf = (1.0 - mask_leaf)\n",
    "        \n",
    "        mask_d_2 = tf.expand_dims(mask_d, 2)\n",
    "        mask_e_1 = tf.expand_dims(mask_e, 1)\n",
    "        \n",
    "        loss_arc = loss_arc * mask_d_2 * mask_e_1\n",
    "        coverage = coverage * mask_d_2 * mask_e_1\n",
    "        loss_type = loss_type * mask_d_2\n",
    "        mask_leaf = mask_leaf * mask_d\n",
    "        mask_non_leaf = mask_non_leaf * mask_d\n",
    "        num_leaf = tf.reduce_sum(mask_leaf)\n",
    "        num_non_leaf = tf.reduce_sum(mask_non_leaf)\n",
    "        head_index = tf.tile(tf.expand_dims(tf.range(0, max_len_d), 1), [1, batch])\n",
    "        \n",
    "        t = tf.transpose(children)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0),\n",
    "                                               tf.expand_dims(head_index, axis = 0),\n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        loss_arc = tf.gather_nd(loss_arc, concatenated)\n",
    "        \n",
    "        t = tf.transpose(stacked_types)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0),\n",
    "                                               tf.expand_dims(head_index, axis = 0),\n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        loss_type = tf.gather_nd(loss_type, concatenated)\n",
    "        \n",
    "        loss_arc_leaf = loss_arc * mask_leaf\n",
    "        loss_arc_non_leaf = loss_arc * mask_non_leaf\n",
    "\n",
    "        loss_type_leaf = loss_type * mask_leaf\n",
    "        loss_type_non_leaf = loss_type * mask_non_leaf\n",
    "        \n",
    "        loss_cov = tf.clip_by_value(coverage - 2.0, 0.0, 100.0)\n",
    "        \n",
    "        return (tf.reduce_sum(-loss_arc_leaf) / num_leaf, \n",
    "                tf.reduce_sum(-loss_arc_non_leaf) / num_non_leaf,\n",
    "                tf.reduce_sum(-loss_type_leaf) / num_leaf, \n",
    "                tf.reduce_sum(-loss_type_non_leaf) / num_non_leaf,\n",
    "                tf.reduce_sum(loss_cov) / (num_leaf + num_non_leaf), \n",
    "                num_leaf, \n",
    "                num_non_leaf)\n",
    "    \n",
    "class Model:\n",
    "    def __init__(self, learning_rate = 1e-3, cov = 0.0):\n",
    "        self.stackpointer = StackPointer(word_dim = 128, \n",
    "                            num_words = len(word2idx), \n",
    "                            char_dim = 128, \n",
    "                            num_chars = len(char2idx), \n",
    "                            num_filters = 128, \n",
    "                            kernel_size = 3,\n",
    "                            input_size_decoder = 256, \n",
    "                            hidden_size = 256, \n",
    "                            encoder_layers = 1, \n",
    "                            decoder_layers = 1,\n",
    "                            num_labels = len(tag2idx), \n",
    "                            arc_space = 128, \n",
    "                            type_space = 128)\n",
    "        self.words = tf.placeholder(tf.int32, (None, None))\n",
    "        self.chars = tf.placeholder(tf.int32, (None, None, None))\n",
    "        self.heads = tf.placeholder(tf.int32, (None, None))\n",
    "        self.stacked_heads = tf.placeholder(tf.int32, (None, None))\n",
    "        self.siblings = tf.placeholder(tf.int32, (None, None))\n",
    "        self.childrens = tf.placeholder(tf.int32, (None, None))\n",
    "        self.stacked_types = tf.placeholder(tf.int32, (None, None))\n",
    "        self.mask_e = tf.placeholder(tf.float32, (None, None))\n",
    "        self.mask_d = tf.placeholder(tf.float32, (None, None))\n",
    "        loss_arc_leaf, loss_arc_non_leaf, \\\n",
    "        loss_type_leaf, loss_type_non_leaf, \\\n",
    "        loss_cov, num_leaf, num_non_leaf = self.stackpointer.loss(self.words, self.chars, self.heads, \n",
    "                               self.stacked_heads, self.childrens, \n",
    "                               self.childrens, self.stacked_types,\n",
    "                               self.mask_e, self.mask_d)\n",
    "        loss_arc = loss_arc_leaf + loss_arc_non_leaf\n",
    "        loss_type = loss_type_leaf + loss_type_non_leaf\n",
    "        self.cost = loss_arc + loss_type + cov * loss_cov\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0929 11:43:26.817160 4504966592 deprecation.py:323] From <ipython-input-16-2a40f1e6112b>:72: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0929 11:43:26.824561 4504966592 deprecation.py:323] From <ipython-input-16-2a40f1e6112b>:79: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0929 11:43:26.974738 4504966592 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0929 11:43:27.224898 4504966592 deprecation.py:323] From <ipython-input-16-2a40f1e6112b>:108: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0929 11:43:27.963778 4504966592 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, ?) (?, ?, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0929 11:43:30.820279 4504966592 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_order = model.stackpointer.prior_order\n",
    "\n",
    "def decode_sentence(output_enc, arc_c, type_c, hx, beam, length, ordered, leading_symbolic):\n",
    "    def valid_hyp(base_id, child_id, head):\n",
    "        if constraints[base_id, child_id]:\n",
    "            return False\n",
    "        elif not ordered or prior_order == PriorOrder.DEPTH or child_orders[base_id, head] == 0:\n",
    "            return True\n",
    "        elif self.prior_order == PriorOrder.LEFT2RIGTH:\n",
    "            return child_id > child_orders[base_id, head]\n",
    "        else:\n",
    "            if child_id < head:\n",
    "                return child_id < child_orders[base_id, head] < head\n",
    "            else:\n",
    "                return child_id > child_orders[base_id, head]\n",
    "        \n",
    "    length = output_enc.shape[0] if length is None else length\n",
    "    hx = tuple([hx])\n",
    "            \n",
    "    stacked_heads = [[0] for _ in range(beam)]\n",
    "    grand_parents = [[0] for _ in range(beam)]\n",
    "    siblings = [[0] for _ in range(beam)]\n",
    "    children = np.zeros((beam, 2 * length - 1))\n",
    "    stacked_types = np.zeros((beam, 2 * length - 1))\n",
    "    \n",
    "    children = np.zeros((beam, 2 * length - 1))\n",
    "    stacked_types = np.zeros((beam, 2 * length - 1))\n",
    "    hypothesis_scores = [0]\n",
    "    constraints = np.zeros([beam, length], dtype=np.bool)\n",
    "    constraints[:, 0] = True\n",
    "    child_orders = np.zeros([beam, length], dtype=np.int64)\n",
    "\n",
    "    new_stacked_heads = [[] for _ in range(beam)]\n",
    "    new_grand_parents = [[] for _ in range(beam)]\n",
    "    new_siblings = [[] for _ in range(beam)]\n",
    "    new_skip_connects = [[] for _ in range(beam)]\n",
    "    new_children = np.zeros((beam, 2 * length - 1))\n",
    "    new_stacked_types = np.zeros((beam, 2 * length - 1))\n",
    "    num_hyp = 1\n",
    "    num_step = 2 * length - 1\n",
    "    for t in range(num_step):\n",
    "        heads = np.array([stacked_heads[i][-1] for i in range(num_hyp)])\n",
    "        gpars = np.array([grand_parents[i][-1] for i in range(num_hyp)])\n",
    "        sibs = np.array([siblings[i].pop() for i in range(num_hyp)])\n",
    "        src_encoding = output_enc[heads]\n",
    "        mask_sibs = np.expand_dims((np.array(sibs) != 0).astype(np.float32), axis = 1)\n",
    "        output_enc_sibling = output_enc[sibs] * mask_sibs\n",
    "        src_encoding = src_encoding + output_enc_sibling\n",
    "        output_enc_gpar = output_enc[gpars]\n",
    "        src_encoding = src_encoding + output_enc_gpar\n",
    "        src_encoding = tf.nn.elu(model.stackpointer.src_dense(src_encoding))\n",
    "        output_dec, hx = model.stackpointer.decoder(src_encoding, hx)\n",
    "        arc_h = tf.nn.elu(model.stackpointer.arc_h(tf.expand_dims(output_dec, axis = 1)))\n",
    "        type_h = tf.nn.elu(model.stackpointer.type_h(output_dec))\n",
    "        out_arc = model.stackpointer.attention.forward(arc_h, tf.expand_dims(arc_c, 0))\n",
    "        out_arc = tf.squeeze(tf.squeeze(out_arc, axis = 1), axis = 1)\n",
    "        hyp_scores, type_h = sess.run([tf.nn.log_softmax(out_arc, axis = 1), type_h])\n",
    "        new_hypothesis_scores = np.expand_dims(hypothesis_scores[:num_hyp], axis = 1) + hyp_scores\n",
    "        new_hypothesis_scores = new_hypothesis_scores.reshape((-1))\n",
    "        hyp_index = np.argsort(new_hypothesis_scores)[::-1]\n",
    "        new_hypothesis_scores = np.sort(new_hypothesis_scores)[::-1]\n",
    "        base_index = (hyp_index // length)\n",
    "        child_index = hyp_index % length\n",
    "        cc = 0\n",
    "        ids = []\n",
    "        new_constraints = np.zeros([beam, length], dtype=np.bool)\n",
    "        new_child_orders = np.zeros([beam, length], dtype=np.int64)\n",
    "        for id_ in range(num_hyp * length):\n",
    "            base_id = base_index[id_]\n",
    "            if base_id:\n",
    "                ids.append(id_)\n",
    "                continue\n",
    "            child_id = child_index[id_]\n",
    "            head = heads[base_id]\n",
    "            new_hyp_score = new_hypothesis_scores[id_]\n",
    "            if child_id == head:\n",
    "                if head != 0 or t + 1 == num_step:\n",
    "                    new_constraints[cc] = constraints[base_id]\n",
    "                    new_child_orders[cc] = child_orders[base_id]\n",
    "\n",
    "                    new_stacked_heads[cc] = [stacked_heads[base_id][i] for i in range(len(stacked_heads[base_id]))]\n",
    "                    new_stacked_heads[cc].pop()\n",
    "\n",
    "                    new_grand_parents[cc] = [grand_parents[base_id][i] for i in range(len(grand_parents[base_id]))]\n",
    "                    new_grand_parents[cc].pop()\n",
    "\n",
    "                    new_siblings[cc] = [siblings[base_id][i] for i in range(len(siblings[base_id]))]\n",
    "\n",
    "                    new_children[cc] = children[base_id]\n",
    "                    new_children[cc, t] = child_id\n",
    "\n",
    "                    hypothesis_scores[cc] = new_hyp_score\n",
    "                    ids.append(id_)\n",
    "                    cc += 1\n",
    "            elif valid_hyp(base_id, child_id, head):\n",
    "                new_constraints[cc] = constraints[base_id]\n",
    "                new_constraints[cc, child_id] = True\n",
    "\n",
    "                new_child_orders[cc] = child_orders[base_id]\n",
    "                new_child_orders[cc, head] = child_id\n",
    "\n",
    "                new_stacked_heads[cc] = [stacked_heads[base_id][i] for i in range(len(stacked_heads[base_id]))]\n",
    "                new_stacked_heads[cc].append(child_id)\n",
    "\n",
    "                new_grand_parents[cc] = [grand_parents[base_id][i] for i in range(len(grand_parents[base_id]))]\n",
    "                new_grand_parents[cc].append(head)\n",
    "\n",
    "                new_siblings[cc] = [siblings[base_id][i] for i in range(len(siblings[base_id]))]\n",
    "                new_siblings[cc].append(child_id)\n",
    "                new_siblings[cc].append(0)\n",
    "\n",
    "                new_children[cc] = children[base_id]\n",
    "                new_children[cc, t] = child_id\n",
    "\n",
    "                hypothesis_scores[cc] = new_hyp_score\n",
    "                ids.append(id_)\n",
    "                cc += 1\n",
    "                    \n",
    "            if cc == beam:\n",
    "                break\n",
    "            \n",
    "        num_hyp = len(ids)\n",
    "        if num_hyp == 0:\n",
    "            return None\n",
    "        else:\n",
    "            index = np.array(ids)\n",
    "        base_index = base_index[index]\n",
    "        child_index = child_index[index]\n",
    "        out_type = model.stackpointer.bilinear.forward(type_h[base_index], type_c[child_index])\n",
    "        hyp_type_scores = sess.run(tf.nn.log_softmax(out_type, axis = 1))\n",
    "        hyp_types = np.argmax(hyp_type_scores, axis = 1)\n",
    "        hyp_type_scores = np.max(hyp_type_scores, axis = 1)\n",
    "        hypothesis_scores[:num_hyp] = hypothesis_scores[:num_hyp] + hyp_type_scores\n",
    "\n",
    "        for i in range(num_hyp):\n",
    "            base_id = base_index[i]\n",
    "            new_stacked_types[i] = stacked_types[base_id]\n",
    "            new_stacked_types[i, t] = hyp_types[i]\n",
    "\n",
    "        stacked_heads = [[new_stacked_heads[i][j] for j in range(len(new_stacked_heads[i]))] for i in range(num_hyp)]\n",
    "        grand_parents = [[new_grand_parents[i][j] for j in range(len(new_grand_parents[i]))] for i in range(num_hyp)]\n",
    "        siblings = [[new_siblings[i][j] for j in range(len(new_siblings[i]))] for i in range(num_hyp)]\n",
    "        constraints = new_constraints\n",
    "        child_orders = new_child_orders\n",
    "        children = np.copy(new_children)\n",
    "        stacked_types = np.copy(new_stacked_types)\n",
    "        \n",
    "    children = children[0].astype(np.int32)\n",
    "    stacked_types = stacked_types[0].astype(np.int32)\n",
    "    heads = np.zeros(length, dtype=np.int32)\n",
    "    types = np.zeros(length, dtype=np.int32)\n",
    "    stack = [0]\n",
    "    for i in range(num_step):\n",
    "        head = stack[-1]\n",
    "        child = children[i]\n",
    "        type_ = stacked_types[i]\n",
    "        if child != head:\n",
    "            heads[child] = head\n",
    "            types[child] = type_\n",
    "            stack.append(child)\n",
    "        else:\n",
    "            stacked_types[i] = 0\n",
    "            stack.pop()\n",
    "\n",
    "    return heads, types, length, children, stacked_types   \n",
    "        \n",
    "def decode(input_word, input_char, length = None, beam = 1, leading_symbolic=0, ordered=True):\n",
    "    output, hn = model.stackpointer.encode(input_word, input_char)\n",
    "    arc_c, type_c, output, hn = sess.run([tf.nn.elu(model.stackpointer.arc_c(output)), \n",
    "                              tf.nn.elu(model.stackpointer.type_c(output)),\n",
    "                              output, hn])\n",
    "    batch, max_len_e, _ = output.shape\n",
    "\n",
    "    heads = np.zeros([batch, max_len_e], dtype=np.int32)\n",
    "    types = np.zeros([batch, max_len_e], dtype=np.int32)\n",
    "\n",
    "    children = np.zeros([batch, 2 * max_len_e - 1], dtype=np.int32)\n",
    "    stack_types = np.zeros([batch, 2 * max_len_e - 1], dtype=np.int32)\n",
    "    \n",
    "    for b in range(batch):\n",
    "        sent_len = None if length is None else length[b]\n",
    "        state = tf.nn.rnn_cell.LSTMStateTuple(hn[0].c[b:b+1], hn[0].h[b:b+1])\n",
    "        preds = decode_sentence(output[b], arc_c[b], type_c[b], state, beam, sent_len, ordered, leading_symbolic)\n",
    "        if preds is None:\n",
    "            preds = decode_sentence(output[b], arc_c[b], type_c[b], state, beam, \n",
    "                                         sent_len, False, leading_symbolic)\n",
    "        hids, tids, sent_len, chids, stids = preds\n",
    "        heads[b, :sent_len] = hids\n",
    "        types[b, :sent_len] = tids\n",
    "\n",
    "        children[b, :2 * sent_len - 1] = chids\n",
    "        stack_types[b, :2 * sent_len - 1] = stids\n",
    "\n",
    "    return heads, types, children, stack_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_w = wid_inputs[:2]\n",
    "batch_c = cid_inputs[:2]\n",
    "batch_heads = hid_inputs[:2]\n",
    "batch_stacked_heads = stack_hid_inputs[:2]\n",
    "batch_siblings = ssid_inputs[:2]\n",
    "batch_children = chid_inputs[:2]\n",
    "batch_stacked_types = stack_tid_inputs[:2]\n",
    "batch_e = masks_e[:2]\n",
    "batch_d = masks_d[:2]\n",
    "batch_types = tid_inputs[:2]\n",
    "batch_len = np.count_nonzero(batch_w, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 1.9 s, total: 1min 17s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# tested in macbook Air\n",
    "NUM_SYMBOLIC_TAGS = 3\n",
    "heads_pred, types_pred, _, _ = decode(batch_w, batch_c, leading_symbolic = NUM_SYMBOLIC_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32),\n",
       " array([[ 0, 33, 33, 29, 29, 23, 29, 12, 30, 30],\n",
       "        [ 0, 23, 25, 33, 30, 30, 30, 30, 30, 30]], dtype=int32))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads_pred, types_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0]], dtype=int32),\n",
       " array([[ 0, 33, 33]], dtype=int32),\n",
       " array([[1, 1, 2, 2, 0]], dtype=int32),\n",
       " array([[33,  0, 33,  0,  0]], dtype=int32))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(batch_w[1:2,:3], batch_c[1:2,:3], leading_symbolic = NUM_SYMBOLIC_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(words, heads_pred, types_pred, heads, types, lengths,\n",
    "             symbolic_root=False, symbolic_end=False):\n",
    "    batch_size, _ = words.shape\n",
    "    ucorr = 0.\n",
    "    lcorr = 0.\n",
    "    total = 0.\n",
    "    ucomplete_match = 0.\n",
    "    lcomplete_match = 0.\n",
    "\n",
    "    corr_root = 0.\n",
    "    total_root = 0.\n",
    "    start = 1 if symbolic_root else 0\n",
    "    end = 1 if symbolic_end else 0\n",
    "    for i in range(batch_size):\n",
    "        ucm = 1.\n",
    "        lcm = 1.\n",
    "        for j in range(start, lengths[i] - end):\n",
    "\n",
    "            total += 1\n",
    "            if heads[i, j] == heads_pred[i, j]:\n",
    "                ucorr += 1\n",
    "                if types[i, j] == types_pred[i, j]:\n",
    "                    lcorr += 1\n",
    "                else:\n",
    "                    lcm = 0\n",
    "            else:\n",
    "                ucm = 0\n",
    "                lcm = 0\n",
    "\n",
    "            if heads[i, j] == 0:\n",
    "                total_root += 1\n",
    "                corr_root += 1 if heads_pred[i, j] == 0 else 0\n",
    "\n",
    "        ucomplete_match += ucm\n",
    "        lcomplete_match += lcm\n",
    "\n",
    "    return (ucorr, lcorr, total, ucomplete_match, lcomplete_match), \\\n",
    "           (corr_root, total_root), batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4.0, 0.0, 11.0, 0.0, 0.0), (4.0, 4.0), 2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(batch_w, heads_pred, types_pred, batch_heads, batch_types, batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
