{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\n",
    "# !wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\n",
    "# !wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya\n",
    "import re\n",
    "from malaya.texts._text_functions import split_into_sentences\n",
    "from malaya.texts import _regex\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "tokenizer = malaya.preprocessing._tokenizer\n",
    "splitter = split_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def preprocessing(w):\n",
    "    if is_number_regex(w):\n",
    "        return '<NUM>'\n",
    "    elif re.match(_regex._money, w):\n",
    "        return '<MONEY>'\n",
    "    elif re.match(_regex._date, w):\n",
    "        return '<DATE>'\n",
    "    elif re.match(_regex._expressions['email'], w):\n",
    "        return '<EMAIL>'\n",
    "    elif re.match(_regex._expressions['url'], w):\n",
    "        return '<URL>'\n",
    "    else:\n",
    "        w = ''.join(''.join(s)[:2] for _, s in itertools.groupby(w))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'PAD': 0,'UNK':1, '_ROOT': 2}\n",
    "tag2idx = {'PAD': 0, '_<ROOT>': 1}\n",
    "char2idx = {'PAD': 0,'UNK':1, '_ROOT': 2}\n",
    "word_idx = 3\n",
    "tag_idx = 2\n",
    "char_idx = 3\n",
    "\n",
    "special_tokens = ['<NUM>', '<MONEY>', '<DATE>', '<URL>', '<EMAIL>']\n",
    "\n",
    "for t in special_tokens:\n",
    "    word2idx[t] = word_idx\n",
    "    word_idx += 1\n",
    "    char2idx[t] = char_idx\n",
    "    char_idx += 1\n",
    "    \n",
    "word2idx, char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"_PAD\"\n",
    "PAD_POS = \"_PAD_POS\"\n",
    "PAD_TYPE = \"_<PAD>\"\n",
    "PAD_CHAR = \"_PAD_CHAR\"\n",
    "ROOT = \"_ROOT\"\n",
    "ROOT_POS = \"_ROOT_POS\"\n",
    "ROOT_TYPE = \"_<ROOT>\"\n",
    "ROOT_CHAR = \"_ROOT_CHAR\"\n",
    "END = \"_END\"\n",
    "END_POS = \"_END_POS\"\n",
    "END_TYPE = \"_<END>\"\n",
    "END_CHAR = \"_END_CHAR\"\n",
    "\n",
    "def process_corpus(corpus, until = None):\n",
    "    global word2idx, tag2idx, char2idx, word_idx, tag_idx, char_idx\n",
    "    sentences, words, depends, labels, pos, chars = [], [], [], [], [], []\n",
    "    temp_sentence, temp_word, temp_depend, temp_label, temp_pos = [], [], [], [], []\n",
    "    first_time = True\n",
    "    for sentence in corpus:\n",
    "        try:\n",
    "            if len(sentence):\n",
    "                if sentence[0] == '#':\n",
    "                    continue\n",
    "                if first_time:\n",
    "                    print(sentence)\n",
    "                    first_time = False\n",
    "                sentence = sentence.split('\\t')\n",
    "                for c in sentence[1]:\n",
    "                    if c not in char2idx:\n",
    "                        char2idx[c] = char_idx\n",
    "                        char_idx += 1\n",
    "                if sentence[7] not in tag2idx:\n",
    "                    tag2idx[sentence[7]] = tag_idx\n",
    "                    tag_idx += 1\n",
    "                sentence[1] = preprocessing(sentence[1])\n",
    "                if sentence[1] not in word2idx:\n",
    "                    word2idx[sentence[1]] = word_idx\n",
    "                    word_idx += 1\n",
    "                temp_word.append(word2idx[sentence[1]])\n",
    "                temp_depend.append(int(sentence[6]))\n",
    "                temp_label.append(tag2idx[sentence[7]])\n",
    "                temp_sentence.append(sentence[1])\n",
    "                temp_pos.append(sentence[3])\n",
    "            else:\n",
    "                if len(temp_sentence) < 2 or len(temp_word) != len(temp_label):\n",
    "                    temp_word = []\n",
    "                    temp_depend = []\n",
    "                    temp_label = []\n",
    "                    temp_sentence = []\n",
    "                    temp_pos = []\n",
    "                    continue\n",
    "                words.append([word2idx['_ROOT']] + temp_word)\n",
    "                depends.append([0] + temp_depend)\n",
    "                labels.append([tag2idx['_<ROOT>']] + temp_label)\n",
    "                sentences.append([ROOT] + temp_sentence)\n",
    "                pos.append([ROOT_POS] + temp_pos)\n",
    "                char_ = [[char2idx['_ROOT']]]\n",
    "                for w in temp_sentence:\n",
    "                    if w in char2idx:\n",
    "                        char_.append([char2idx[w]])\n",
    "                    else:\n",
    "                        char_.append([char2idx[c] for c in w])\n",
    "                chars.append(char_)\n",
    "                temp_word = []\n",
    "                temp_depend = []\n",
    "                temp_label = []\n",
    "                temp_sentence = []\n",
    "                temp_pos = []\n",
    "        except Exception as e:\n",
    "            print(e, sentence)\n",
    "    return sentences[:-1], words[:-1], depends[:-1], labels[:-1], pos[:-1], chars[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _obtain_child_index_for_left2right(heads):\n",
    "    child_ids = [[] for _ in range(len(heads))]\n",
    "    # skip the symbolic root.\n",
    "    for child in range(1, len(heads)):\n",
    "        head = heads[child]\n",
    "        child_ids[head].append(child)\n",
    "    return child_ids\n",
    "\n",
    "\n",
    "def _obtain_child_index_for_inside_out(heads):\n",
    "    child_ids = [[] for _ in range(len(heads))]\n",
    "    for head in range(len(heads)):\n",
    "        # first find left children inside-out\n",
    "        for child in reversed(range(1, head)):\n",
    "            if heads[child] == head:\n",
    "                child_ids[head].append(child)\n",
    "        # second find right children inside-out\n",
    "        for child in range(head + 1, len(heads)):\n",
    "            if heads[child] == head:\n",
    "                child_ids[head].append(child)\n",
    "    return child_ids\n",
    "\n",
    "\n",
    "def _obtain_child_index_for_depth(heads, reverse):\n",
    "    def calc_depth(head):\n",
    "        children = child_ids[head]\n",
    "        max_depth = 0\n",
    "        for child in children:\n",
    "            depth = calc_depth(child)\n",
    "            child_with_depth[head].append((child, depth))\n",
    "            max_depth = max(max_depth, depth + 1)\n",
    "        child_with_depth[head] = sorted(child_with_depth[head], key=lambda x: x[1], reverse=reverse)\n",
    "        return max_depth\n",
    "\n",
    "    child_ids = _obtain_child_index_for_left2right(heads)\n",
    "    child_with_depth = [[] for _ in range(len(heads))]\n",
    "    calc_depth(0)\n",
    "    return [[child for child, depth in child_with_depth[head]] for head in range(len(heads))]\n",
    "\n",
    "\n",
    "def _generate_stack_inputs(heads, types, prior_order):\n",
    "    if prior_order == 'deep_first':\n",
    "        child_ids = _obtain_child_index_for_depth(heads, True)\n",
    "    elif prior_order == 'shallow_first':\n",
    "        child_ids = _obtain_child_index_for_depth(heads, False)\n",
    "    elif prior_order == 'left2right':\n",
    "        child_ids = _obtain_child_index_for_left2right(heads)\n",
    "    elif prior_order == 'inside_out':\n",
    "        child_ids = _obtain_child_index_for_inside_out(heads)\n",
    "    else:\n",
    "        raise ValueError('Unknown prior order: %s' % prior_order)\n",
    "\n",
    "    stacked_heads = []\n",
    "    children = []\n",
    "    siblings = []\n",
    "    stacked_types = []\n",
    "    skip_connect = []\n",
    "    prev = [0 for _ in range(len(heads))]\n",
    "    sibs = [0 for _ in range(len(heads))]\n",
    "    stack = [0]\n",
    "    position = 1\n",
    "    while len(stack) > 0:\n",
    "        head = stack[-1]\n",
    "        stacked_heads.append(head)\n",
    "        siblings.append(sibs[head])\n",
    "        child_id = child_ids[head]\n",
    "        skip_connect.append(prev[head])\n",
    "        prev[head] = position\n",
    "        if len(child_id) == 0:\n",
    "            children.append(head)\n",
    "            sibs[head] = 0\n",
    "            stacked_types.append(tag2idx['PAD'])\n",
    "            stack.pop()\n",
    "        else:\n",
    "            child = child_id.pop(0)\n",
    "            children.append(child)\n",
    "            sibs[head] = child\n",
    "            stack.append(child)\n",
    "            stacked_types.append(types[child])\n",
    "        position += 1\n",
    "\n",
    "    return stacked_heads, children, siblings, stacked_types, skip_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_buckets = [10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100, 140]\n",
    "\n",
    "def process_data(corpus, batch_size = 32):\n",
    "    sentences, words, depends, labels, pos, chars = process_corpus(corpus)\n",
    "    print(len(sentences), len(words), len(depends), len(labels), len(chars))\n",
    "\n",
    "    data = [[] for _ in _buckets]\n",
    "    max_char_length = [0 for _ in _buckets]\n",
    "\n",
    "    for i in range(len(depends)):\n",
    "        for bucket_id, bucket_size in enumerate(_buckets):\n",
    "            if len(words[i]) < bucket_size:\n",
    "                stacked_heads, children, siblings, stacked_types, skip_connect \\\n",
    "                = _generate_stack_inputs(depends[i], labels[i], 'deep_first')\n",
    "                data[bucket_id].append([words[i], pos[i], depends[i], labels[i], chars[i],\n",
    "                                        stacked_heads, children, siblings, stacked_types, skip_connect])\n",
    "                max_len = max([len(char_seq) for char_seq in chars[i]])\n",
    "                if max_char_length[bucket_id] < max_len:\n",
    "                    max_char_length[bucket_id] = max_len\n",
    "                break\n",
    "    \n",
    "    bucket_sizes = [len(data[b]) for b in range(len(_buckets))]\n",
    "    X = []\n",
    "    for bucket_id in range(len(_buckets)):\n",
    "        bucket_length = _buckets[bucket_id]\n",
    "        bucket_size = bucket_sizes[bucket_id]\n",
    "        if not bucket_size:\n",
    "            X.append([])\n",
    "            continue\n",
    "        char_length = max_char_length[bucket_id]\n",
    "\n",
    "        wid_inputs = np.zeros([bucket_size, bucket_length], dtype=np.int32)\n",
    "        cid_inputs = np.zeros([bucket_size, bucket_length, char_length], dtype=np.int32)\n",
    "        hid_inputs = np.zeros([bucket_size, bucket_length], dtype=np.int32)\n",
    "        tid_inputs = np.zeros([bucket_size, bucket_length], dtype=np.int32)\n",
    "\n",
    "        stack_hid_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "        chid_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "        ssid_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "        stack_tid_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "        skip_connect_inputs = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.int32)\n",
    "\n",
    "        masks_e = np.zeros([bucket_size, bucket_length], dtype=np.float32)\n",
    "        masks_d = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.float32)\n",
    "        lengths_d = np.empty(bucket_size, dtype=np.int32)\n",
    "\n",
    "        for i, inst in enumerate(data[bucket_id]):\n",
    "            w, p, d, l, ch, stack_hids, chids, ssids, stack_tids, skip_ids = inst\n",
    "            inst_size = len(w)\n",
    "            wid_inputs[i, :inst_size] = w\n",
    "            for c, cids in enumerate(ch):\n",
    "                cid_inputs[i, c, :len(cids)] = cids\n",
    "            tid_inputs[i, :inst_size] = l\n",
    "            hid_inputs[i, :inst_size] = d\n",
    "            masks_e[i, :inst_size] = 1.0\n",
    "\n",
    "            inst_size_decoder = 2 * inst_size - 1\n",
    "            stack_hid_inputs[i, :inst_size_decoder] = stack_hids\n",
    "            chid_inputs[i, :inst_size_decoder] = chids\n",
    "            ssid_inputs[i, :inst_size_decoder] = ssids\n",
    "            stack_tid_inputs[i, :inst_size_decoder] = stack_tids\n",
    "            skip_connect_inputs[i, :inst_size_decoder] = skip_ids\n",
    "            masks_d[i, :inst_size_decoder] = 1.0\n",
    "\n",
    "        x = (wid_inputs, cid_inputs, hid_inputs, tid_inputs, stack_hid_inputs, chid_inputs, ssid_inputs,\n",
    "        stack_tid_inputs, masks_e, masks_d)\n",
    "        X.append(x)\n",
    "\n",
    "    train_X = []\n",
    "    for X_ in X:\n",
    "        if not len(X_):\n",
    "            continue\n",
    "        wid_inputs, cid_inputs, hid_inputs, tid_inputs, stack_hid_inputs, chid_inputs, ssid_inputs, stack_tid_inputs, masks_e, masks_d = X_\n",
    "        for k in range(0, len(wid_inputs), batch_size):\n",
    "            index = min(k + batch_size, len(wid_inputs))\n",
    "            batch_w = wid_inputs[k: index]\n",
    "            batch_c = cid_inputs[k: index]\n",
    "            batch_heads = hid_inputs[k: index]\n",
    "            batch_stacked_heads = stack_hid_inputs[k: index]\n",
    "            batch_siblings = ssid_inputs[k: index]\n",
    "            batch_children = chid_inputs[k: index]\n",
    "            batch_stacked_types = stack_tid_inputs[k: index]\n",
    "            batch_e = masks_e[k: index]\n",
    "            batch_d = masks_d[k: index]\n",
    "            batch_types = tid_inputs[k: index]\n",
    "            batch_len = np.count_nonzero(batch_w, axis = 1)\n",
    "            x = (batch_w, batch_c, batch_heads, batch_stacked_heads, batch_siblings, batch_children,\n",
    "            batch_stacked_types, batch_e, batch_d, batch_types, batch_len)\n",
    "            train_X.append(x)\n",
    "            \n",
    "    print('trainable batch size', len(train_X))\n",
    "    return train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-dev.conllu') as fopen:\n",
    "    dev = fopen.read().split('\\n')\n",
    "\n",
    "test = process_data(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-test.conllu') as fopen:\n",
    "    dev = fopen.read().split('\\n')\n",
    "\n",
    "test.extend(process_data(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-train.conllu') as fopen:\n",
    "    train = fopen.read().split('\\n')\n",
    "\n",
    "train = process_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "idx2tag = {v:k for k, v in tag2idx.items()}\n",
    "len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class PriorOrder(Enum):\n",
    "    DEPTH = 0\n",
    "    INSIDE_OUT = 1\n",
    "    LEFT2RIGTH = 2\n",
    "\n",
    "class BiAAttention:\n",
    "    def __init__(self, input_size_encoder, input_size_decoder, num_labels):\n",
    "        self.input_size_encoder = input_size_encoder\n",
    "        self.input_size_decoder = input_size_decoder\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.W_d = tf.get_variable(\"W_d\", shape=[self.num_labels, self.input_size_decoder],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.W_e = tf.get_variable(\"W_e\", shape=[self.num_labels, self.input_size_encoder],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.U = tf.get_variable(\"U\", shape=[self.num_labels, self.input_size_decoder, self.input_size_encoder],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "    def forward(self, input_d, input_e, mask_d=None, mask_e=None):\n",
    "        batch = tf.shape(input_d)[0]\n",
    "        length_decoder = tf.shape(input_d)[1]\n",
    "        length_encoder = tf.shape(input_e)[1]\n",
    "        out_d = tf.expand_dims(tf.matmul(self.W_d, tf.transpose(input_d, [0, 2, 1])), 3)\n",
    "        out_e = tf.expand_dims(tf.matmul(self.W_e, tf.transpose(input_e, [0, 2, 1])), 2)\n",
    "        output = tf.matmul(tf.expand_dims(input_d, 1), self.U)\n",
    "        output = tf.matmul(output, tf.transpose(tf.expand_dims(input_e, 1), [0, 1, 3, 2]))\n",
    "        \n",
    "        output = output + out_d + out_e\n",
    "        \n",
    "        if mask_d is not None:\n",
    "            d = tf.expand_dims(tf.expand_dims(mask_d, 1), 3)\n",
    "            e = tf.expand_dims(tf.expand_dims(mask_e, 1), 2)\n",
    "            output = output * d * e\n",
    "            \n",
    "        return output\n",
    "    \n",
    "class BiLinear:\n",
    "    def __init__(self, left_features, right_features, out_features):\n",
    "        self.left_features = left_features\n",
    "        self.right_features = right_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.U = tf.get_variable(\"U-bi\", shape=[out_features, left_features, right_features],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.W_l = tf.get_variable(\"Wl\", shape=[out_features, left_features],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.W_r = tf.get_variable(\"Wr\", shape=[out_features, right_features],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    def forward(self, input_left, input_right):\n",
    "        left_size = tf.shape(input_left)\n",
    "        output_shape = tf.concat([left_size[:-1], [self.out_features]], axis = 0)\n",
    "        batch = tf.cast(tf.reduce_prod(left_size[:-1]), tf.int32)\n",
    "        input_left = tf.reshape(input_left, (batch, self.left_features))\n",
    "        input_right = tf.reshape(input_right, (batch, self.right_features))\n",
    "        tiled = tf.tile(tf.expand_dims(input_left, axis = 0), (self.out_features,1,1))\n",
    "        output = tf.transpose(tf.reduce_sum(tf.matmul(tiled, self.U), axis = 2))\n",
    "        output = output + tf.matmul(input_left, tf.transpose(self.W_l))\\\n",
    "        + tf.matmul(input_right, tf.transpose(self.W_r))\n",
    "        \n",
    "        return tf.reshape(output, output_shape)\n",
    "\n",
    "class StackPointer:\n",
    "    def __init__(self, word_dim, num_words, char_dim, num_chars, num_filters, kernel_size,\n",
    "                 input_size_decoder, hidden_size, encoder_layers, decoder_layers,\n",
    "                 num_labels, arc_space, type_space):\n",
    "        \n",
    "        def cells(size, reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size,\n",
    "                                           initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
    "        \n",
    "        self.word_embedd = tf.Variable(tf.random_uniform([num_words, word_dim], -1, 1))\n",
    "        self.char_embedd = tf.Variable(tf.random_uniform([num_chars, char_dim], -1, 1))\n",
    "        self.conv1d = tf.layers.Conv1D(num_filters, kernel_size, 1, padding='VALID')\n",
    "        self.num_labels = num_labels\n",
    "        self.prior_order = PriorOrder.DEPTH\n",
    "        self.encoder = tf.nn.rnn_cell.MultiRNNCell([cells(hidden_size) for _ in range(encoder_layers)])\n",
    "        self.decoder = tf.nn.rnn_cell.MultiRNNCell([cells(hidden_size) for _ in range(decoder_layers)])\n",
    "        \n",
    "        self.src_dense = tf.layers.Dense(hidden_size)\n",
    "        self.hx_dense = tf.layers.Dense(hidden_size)\n",
    "\n",
    "        self.arc_h = tf.layers.Dense(arc_space)\n",
    "        self.arc_c = tf.layers.Dense(arc_space)\n",
    "        self.attention = BiAAttention(arc_space, arc_space, 1)\n",
    "\n",
    "        self.type_h = tf.layers.Dense(type_space)\n",
    "        self.type_c = tf.layers.Dense(type_space)\n",
    "        self.bilinear = BiLinear(type_space, type_space, self.num_labels)\n",
    "        \n",
    "    def encode(self, input_word, input_char):\n",
    "        word = tf.nn.embedding_lookup(self.word_embedd, input_word)\n",
    "        char = tf.nn.embedding_lookup(self.char_embedd, input_char)\n",
    "        b = tf.shape(char)[0]\n",
    "        wl = tf.shape(char)[1]\n",
    "        cl = tf.shape(char)[2]\n",
    "        d = char.shape[3]\n",
    "        char = tf.reshape(char, [b * wl, cl, d])\n",
    "        char = tf.reduce_max(self.conv1d(char), axis = 1)\n",
    "        char = tf.nn.tanh(char)\n",
    "        d = char.shape[-1]\n",
    "        char = tf.reshape(char, [b, wl, d])\n",
    "        \n",
    "        src_encoding = tf.concat([word, char], axis=2)\n",
    "        output, hn = tf.nn.dynamic_rnn(self.encoder, src_encoding, dtype = tf.float32,\n",
    "                                      scope = 'encoder')\n",
    "        return output, hn\n",
    "    \n",
    "    def decode(self, output_encoder, heads, heads_stack, siblings, hn):\n",
    "        batch = tf.shape(output_encoder)[0]\n",
    "        batch_index = tf.range(0, batch)\n",
    "        t = tf.transpose(heads_stack)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        src_encoding = tf.gather_nd(output_encoder, concatenated)\n",
    "        \n",
    "        mask_sibs = tf.expand_dims(tf.cast(tf.not_equal(siblings, 0), tf.float32), axis = 2)\n",
    "        t = tf.transpose(siblings)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        output_enc_sibling = tf.gather_nd(output_encoder, concatenated) * mask_sibs\n",
    "        src_encoding = src_encoding + output_enc_sibling\n",
    "        \n",
    "        t = tf.transpose(heads_stack)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                       tf.expand_dims(t, axis = 0)],axis = 0))\n",
    "        g = tf.transpose(tf.gather_nd(heads, concatenated))\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(g))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                               tf.expand_dims(g, axis = 0)],axis = 0))\n",
    "        output_enc_gpar = tf.gather_nd(output_encoder, concatenated)\n",
    "        src_encoding = src_encoding + output_enc_gpar\n",
    "        \n",
    "        src_encoding = tf.nn.elu(self.src_dense(src_encoding))\n",
    "        output, hn = tf.nn.dynamic_rnn(self.decoder, src_encoding, dtype = tf.float32,\n",
    "                                      initial_state = hn,\n",
    "                                      scope = 'decoder')\n",
    "        return output, hn\n",
    "    \n",
    "    def loss(self, input_word, input_char, \n",
    "             heads, stacked_heads, children, siblings, stacked_types,\n",
    "             mask_e, mask_d,\n",
    "             label_smooth = 1.0):\n",
    "        \n",
    "        output_enc, hn_enc = self.encode(input_word, input_char)\n",
    "        arc_c = tf.nn.elu(self.arc_c(output_enc))\n",
    "        type_c = tf.nn.elu(self.type_c(output_enc))\n",
    "        \n",
    "        output_dec, _ = self.decode(output_enc, heads, stacked_heads, siblings, hn_enc)\n",
    "        arc_h = tf.nn.elu(self.arc_h(output_dec))\n",
    "        type_h = tf.nn.elu(self.type_h(output_dec))\n",
    "        \n",
    "        max_len_d = tf.shape(arc_h)[1]\n",
    "        \n",
    "        out_arc = tf.squeeze(self.attention.forward(arc_h, arc_c, mask_d=mask_d, mask_e=mask_e), axis = 1)\n",
    "        batch = tf.shape(arc_c)[0]\n",
    "        max_len_e = tf.shape(arc_c)[1]\n",
    "        batch_index = tf.range(0, batch)\n",
    "        \n",
    "        t = tf.transpose(children)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0), \n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        type_c = tf.gather_nd(type_c, concatenated)\n",
    "        out_type = self.bilinear.forward(type_h, type_c)\n",
    "        print(out_arc.shape,out_type.shape)\n",
    "        \n",
    "        minus_inf = -1e8\n",
    "        minus_mask_d = (1 - mask_d) * minus_inf\n",
    "        minus_mask_e = (1 - mask_e) * minus_inf\n",
    "        \n",
    "        out_arc = out_arc + tf.expand_dims(minus_mask_d, 2) + tf.expand_dims(minus_mask_e, 1)\n",
    "        loss_arc = tf.nn.log_softmax(out_arc, axis = 2)\n",
    "        loss_type = tf.nn.log_softmax(out_type, axis = 2)\n",
    "        coverage = tf.cumsum(tf.exp(loss_arc), axis = 1)\n",
    "        \n",
    "        mask_leaf = tf.cast(tf.equal(children, stacked_heads), tf.float32)\n",
    "        mask_non_leaf = (1.0 - mask_leaf)\n",
    "        \n",
    "        mask_d_2 = tf.expand_dims(mask_d, 2)\n",
    "        mask_e_1 = tf.expand_dims(mask_e, 1)\n",
    "        \n",
    "        loss_arc = loss_arc * mask_d_2 * mask_e_1\n",
    "        coverage = coverage * mask_d_2 * mask_e_1\n",
    "        loss_type = loss_type * mask_d_2\n",
    "        mask_leaf = mask_leaf * mask_d\n",
    "        mask_non_leaf = mask_non_leaf * mask_d\n",
    "        num_leaf = tf.reduce_sum(mask_leaf)\n",
    "        num_non_leaf = tf.reduce_sum(mask_non_leaf)\n",
    "        head_index = tf.tile(tf.expand_dims(tf.range(0, max_len_d), 1), [1, batch])\n",
    "        \n",
    "        t = tf.transpose(children)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0),\n",
    "                                               tf.expand_dims(head_index, axis = 0),\n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        loss_arc = tf.gather_nd(loss_arc, concatenated)\n",
    "        \n",
    "        t = tf.transpose(stacked_types)\n",
    "        broadcasted = tf.broadcast_to(batch_index, tf.shape(t))\n",
    "        concatenated = tf.transpose(tf.concat([tf.expand_dims(broadcasted, axis = 0),\n",
    "                                               tf.expand_dims(head_index, axis = 0),\n",
    "                                               tf.expand_dims(t, axis = 0)], axis = 0))\n",
    "        loss_type = tf.gather_nd(loss_type, concatenated)\n",
    "        \n",
    "        loss_arc_leaf = loss_arc * mask_leaf\n",
    "        loss_arc_non_leaf = loss_arc * mask_non_leaf\n",
    "\n",
    "        loss_type_leaf = loss_type * mask_leaf\n",
    "        loss_type_non_leaf = loss_type * mask_non_leaf\n",
    "        \n",
    "        loss_cov = tf.clip_by_value(coverage - 2.0, 0.0, 100.0)\n",
    "        \n",
    "        return (tf.reduce_sum(-loss_arc_leaf) / num_leaf, \n",
    "                tf.reduce_sum(-loss_arc_non_leaf) / num_non_leaf,\n",
    "                tf.reduce_sum(-loss_type_leaf) / num_leaf, \n",
    "                tf.reduce_sum(-loss_type_non_leaf) / num_non_leaf,\n",
    "                tf.reduce_sum(loss_cov) / (num_leaf + num_non_leaf), \n",
    "                num_leaf, \n",
    "                num_non_leaf)\n",
    "    \n",
    "class Model:\n",
    "    def __init__(self, learning_rate = 1e-3, cov = 0.0):\n",
    "        self.stackpointer = StackPointer(word_dim = 128, \n",
    "                            num_words = len(word2idx), \n",
    "                            char_dim = 128, \n",
    "                            num_chars = len(char2idx), \n",
    "                            num_filters = 128, \n",
    "                            kernel_size = 3,\n",
    "                            input_size_decoder = 256, \n",
    "                            hidden_size = 256, \n",
    "                            encoder_layers = 1, \n",
    "                            decoder_layers = 1,\n",
    "                            num_labels = len(tag2idx), \n",
    "                            arc_space = 128, \n",
    "                            type_space = 128)\n",
    "        self.words = tf.placeholder(tf.int32, (None, None))\n",
    "        self.chars = tf.placeholder(tf.int32, (None, None, None))\n",
    "        self.heads = tf.placeholder(tf.int32, (None, None))\n",
    "        self.stacked_heads = tf.placeholder(tf.int32, (None, None))\n",
    "        self.siblings = tf.placeholder(tf.int32, (None, None))\n",
    "        self.childrens = tf.placeholder(tf.int32, (None, None))\n",
    "        self.stacked_types = tf.placeholder(tf.int32, (None, None))\n",
    "        self.mask_e = tf.placeholder(tf.float32, (None, None))\n",
    "        self.mask_d = tf.placeholder(tf.float32, (None, None))\n",
    "        loss_arc_leaf, loss_arc_non_leaf, \\\n",
    "        loss_type_leaf, loss_type_non_leaf, \\\n",
    "        loss_cov, num_leaf, num_non_leaf = self.stackpointer.loss(self.words, self.chars, self.heads, \n",
    "                               self.stacked_heads, self.childrens, \n",
    "                               self.siblings, self.stacked_types,\n",
    "                               self.mask_e, self.mask_d)\n",
    "        loss_arc = loss_arc_leaf + loss_arc_non_leaf\n",
    "        loss_type = loss_type_leaf + loss_type_non_leaf\n",
    "        self.cost = loss_arc + loss_type + cov * loss_cov\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_order = model.stackpointer.prior_order\n",
    "\n",
    "def decode_sentence(output_enc, arc_c, type_c, hx, beam, length, ordered, leading_symbolic):\n",
    "    def valid_hyp(base_id, child_id, head):\n",
    "        if constraints[base_id, child_id]:\n",
    "            return False\n",
    "        elif not ordered or prior_order == PriorOrder.DEPTH or child_orders[base_id, head] == 0:\n",
    "            return True\n",
    "        elif self.prior_order == PriorOrder.LEFT2RIGTH:\n",
    "            return child_id > child_orders[base_id, head]\n",
    "        else:\n",
    "            if child_id < head:\n",
    "                return child_id < child_orders[base_id, head] < head\n",
    "            else:\n",
    "                return child_id > child_orders[base_id, head]\n",
    "        \n",
    "    length = output_enc.shape[0] if length is None else length\n",
    "    hx = tuple([hx])\n",
    "            \n",
    "    stacked_heads = [[0] for _ in range(beam)]\n",
    "    grand_parents = [[0] for _ in range(beam)]\n",
    "    siblings = [[0] for _ in range(beam)]\n",
    "    children = np.zeros((beam, 2 * length - 1))\n",
    "    stacked_types = np.zeros((beam, 2 * length - 1))\n",
    "    \n",
    "    children = np.zeros((beam, 2 * length - 1))\n",
    "    stacked_types = np.zeros((beam, 2 * length - 1))\n",
    "    hypothesis_scores = [0]\n",
    "    constraints = np.zeros([beam, length], dtype=np.bool)\n",
    "    constraints[:, 0] = True\n",
    "    child_orders = np.zeros([beam, length], dtype=np.int64)\n",
    "\n",
    "    new_stacked_heads = [[] for _ in range(beam)]\n",
    "    new_grand_parents = [[] for _ in range(beam)]\n",
    "    new_siblings = [[] for _ in range(beam)]\n",
    "    new_skip_connects = [[] for _ in range(beam)]\n",
    "    new_children = np.zeros((beam, 2 * length - 1))\n",
    "    new_stacked_types = np.zeros((beam, 2 * length - 1))\n",
    "    num_hyp = 1\n",
    "    num_step = 2 * length - 1\n",
    "    for t in range(num_step):\n",
    "        heads = np.array([stacked_heads[i][-1] for i in range(num_hyp)])\n",
    "        gpars = np.array([grand_parents[i][-1] for i in range(num_hyp)])\n",
    "        sibs = np.array([siblings[i].pop() for i in range(num_hyp)])\n",
    "        src_encoding = output_enc[heads]\n",
    "        mask_sibs = np.expand_dims((np.array(sibs) != 0).astype(np.float32), axis = 1)\n",
    "        output_enc_sibling = output_enc[sibs] * mask_sibs\n",
    "        src_encoding = src_encoding + output_enc_sibling\n",
    "        output_enc_gpar = output_enc[gpars]\n",
    "        src_encoding = src_encoding + output_enc_gpar\n",
    "        src_encoding = tf.nn.elu(model.stackpointer.src_dense(src_encoding))\n",
    "        output_dec, hx = model.stackpointer.decoder(src_encoding, hx)\n",
    "        arc_h = tf.nn.elu(model.stackpointer.arc_h(tf.expand_dims(output_dec, axis = 1)))\n",
    "        type_h = tf.nn.elu(model.stackpointer.type_h(output_dec))\n",
    "        out_arc = model.stackpointer.attention.forward(arc_h, tf.expand_dims(arc_c, 0))\n",
    "        out_arc = tf.squeeze(tf.squeeze(out_arc, axis = 1), axis = 1)\n",
    "        hyp_scores, type_h = sess.run([tf.nn.log_softmax(out_arc, axis = 1), type_h])\n",
    "        new_hypothesis_scores = np.expand_dims(hypothesis_scores[:num_hyp], axis = 1) + hyp_scores\n",
    "        new_hypothesis_scores = new_hypothesis_scores.reshape((-1))\n",
    "        hyp_index = np.argsort(new_hypothesis_scores)[::-1]\n",
    "        new_hypothesis_scores = np.sort(new_hypothesis_scores)[::-1]\n",
    "        base_index = (hyp_index // length)\n",
    "        child_index = hyp_index % length\n",
    "        cc = 0\n",
    "        ids = []\n",
    "        new_constraints = np.zeros([beam, length], dtype=np.bool)\n",
    "        new_child_orders = np.zeros([beam, length], dtype=np.int64)\n",
    "        for id_ in range(num_hyp * length):\n",
    "            base_id = base_index[id_]\n",
    "            if base_id:\n",
    "                ids.append(id_)\n",
    "                continue\n",
    "            child_id = child_index[id_]\n",
    "            head = heads[base_id]\n",
    "            new_hyp_score = new_hypothesis_scores[id_]\n",
    "            if child_id == head:\n",
    "                if head != 0 or t + 1 == num_step:\n",
    "                    new_constraints[cc] = constraints[base_id]\n",
    "                    new_child_orders[cc] = child_orders[base_id]\n",
    "\n",
    "                    new_stacked_heads[cc] = [stacked_heads[base_id][i] for i in range(len(stacked_heads[base_id]))]\n",
    "                    new_stacked_heads[cc].pop()\n",
    "\n",
    "                    new_grand_parents[cc] = [grand_parents[base_id][i] for i in range(len(grand_parents[base_id]))]\n",
    "                    new_grand_parents[cc].pop()\n",
    "\n",
    "                    new_siblings[cc] = [siblings[base_id][i] for i in range(len(siblings[base_id]))]\n",
    "\n",
    "                    new_children[cc] = children[base_id]\n",
    "                    new_children[cc, t] = child_id\n",
    "\n",
    "                    hypothesis_scores[cc] = new_hyp_score\n",
    "                    ids.append(id_)\n",
    "                    cc += 1\n",
    "            elif valid_hyp(base_id, child_id, head):\n",
    "                new_constraints[cc] = constraints[base_id]\n",
    "                new_constraints[cc, child_id] = True\n",
    "\n",
    "                new_child_orders[cc] = child_orders[base_id]\n",
    "                new_child_orders[cc, head] = child_id\n",
    "\n",
    "                new_stacked_heads[cc] = [stacked_heads[base_id][i] for i in range(len(stacked_heads[base_id]))]\n",
    "                new_stacked_heads[cc].append(child_id)\n",
    "\n",
    "                new_grand_parents[cc] = [grand_parents[base_id][i] for i in range(len(grand_parents[base_id]))]\n",
    "                new_grand_parents[cc].append(head)\n",
    "\n",
    "                new_siblings[cc] = [siblings[base_id][i] for i in range(len(siblings[base_id]))]\n",
    "                new_siblings[cc].append(child_id)\n",
    "                new_siblings[cc].append(0)\n",
    "\n",
    "                new_children[cc] = children[base_id]\n",
    "                new_children[cc, t] = child_id\n",
    "\n",
    "                hypothesis_scores[cc] = new_hyp_score\n",
    "                ids.append(id_)\n",
    "                cc += 1\n",
    "                    \n",
    "            if cc == beam:\n",
    "                break\n",
    "            \n",
    "        num_hyp = len(ids)\n",
    "        if num_hyp == 0:\n",
    "            return None\n",
    "        else:\n",
    "            index = np.array(ids)\n",
    "        base_index = base_index[index]\n",
    "        child_index = child_index[index]\n",
    "        out_type = model.stackpointer.bilinear.forward(type_h[base_index], type_c[child_index])\n",
    "        hyp_type_scores = sess.run(tf.nn.log_softmax(out_type, axis = 1))\n",
    "        hyp_types = np.argmax(hyp_type_scores, axis = 1)\n",
    "        hyp_type_scores = np.max(hyp_type_scores, axis = 1)\n",
    "        hypothesis_scores[:num_hyp] = hypothesis_scores[:num_hyp] + hyp_type_scores\n",
    "\n",
    "        for i in range(num_hyp):\n",
    "            base_id = base_index[i]\n",
    "            new_stacked_types[i] = stacked_types[base_id]\n",
    "            new_stacked_types[i, t] = hyp_types[i]\n",
    "\n",
    "        stacked_heads = [[new_stacked_heads[i][j] for j in range(len(new_stacked_heads[i]))] for i in range(num_hyp)]\n",
    "        grand_parents = [[new_grand_parents[i][j] for j in range(len(new_grand_parents[i]))] for i in range(num_hyp)]\n",
    "        siblings = [[new_siblings[i][j] for j in range(len(new_siblings[i]))] for i in range(num_hyp)]\n",
    "        constraints = new_constraints\n",
    "        child_orders = new_child_orders\n",
    "        children = np.copy(new_children)\n",
    "        stacked_types = np.copy(new_stacked_types)\n",
    "        \n",
    "    children = children[0].astype(np.int32)\n",
    "    stacked_types = stacked_types[0].astype(np.int32)\n",
    "    heads = np.zeros(length, dtype=np.int32)\n",
    "    types = np.zeros(length, dtype=np.int32)\n",
    "    stack = [0]\n",
    "    for i in range(num_step):\n",
    "        head = stack[-1]\n",
    "        child = children[i]\n",
    "        type_ = stacked_types[i]\n",
    "        if child != head:\n",
    "            heads[child] = head\n",
    "            types[child] = type_\n",
    "            stack.append(child)\n",
    "        else:\n",
    "            stacked_types[i] = 0\n",
    "            stack.pop()\n",
    "\n",
    "    return heads, types, length, children, stacked_types   \n",
    "        \n",
    "def decode(input_word, input_char, length = None, beam = 1, leading_symbolic=0, ordered=True):\n",
    "    output, hn = model.stackpointer.encode(input_word, input_char)\n",
    "    arc_c, type_c, output, hn = sess.run([tf.nn.elu(model.stackpointer.arc_c(output)), \n",
    "                              tf.nn.elu(model.stackpointer.type_c(output)),\n",
    "                              output, hn])\n",
    "    batch, max_len_e, _ = output.shape\n",
    "\n",
    "    heads = np.zeros([batch, max_len_e], dtype=np.int32)\n",
    "    types = np.zeros([batch, max_len_e], dtype=np.int32)\n",
    "\n",
    "    children = np.zeros([batch, 2 * max_len_e - 1], dtype=np.int32)\n",
    "    stack_types = np.zeros([batch, 2 * max_len_e - 1], dtype=np.int32)\n",
    "    \n",
    "    for b in range(batch):\n",
    "        sent_len = None if length is None else length[b]\n",
    "        state = tf.nn.rnn_cell.LSTMStateTuple(hn[0].c[b:b+1], hn[0].h[b:b+1])\n",
    "        preds = decode_sentence(output[b], arc_c[b], type_c[b], state, beam, sent_len, ordered, leading_symbolic)\n",
    "        if preds is None:\n",
    "            preds = decode_sentence(output[b], arc_c[b], type_c[b], state, beam, \n",
    "                                         sent_len, False, leading_symbolic)\n",
    "        hids, tids, sent_len, chids, stids = preds\n",
    "        heads[b, :sent_len] = hids\n",
    "        types[b, :sent_len] = tids\n",
    "\n",
    "        children[b, :2 * sent_len - 1] = chids\n",
    "        stack_types[b, :2 * sent_len - 1] = stids\n",
    "\n",
    "    return heads, types, children, stack_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_w = wid_inputs[:2]\n",
    "# batch_c = cid_inputs[:2]\n",
    "# batch_heads = hid_inputs[:2]\n",
    "# batch_stacked_heads = stack_hid_inputs[:2]\n",
    "# batch_siblings = ssid_inputs[:2]\n",
    "# batch_children = chid_inputs[:2]\n",
    "# batch_stacked_types = stack_tid_inputs[:2]\n",
    "# batch_e = masks_e[:2]\n",
    "# batch_d = masks_d[:2]\n",
    "# batch_types = tid_inputs[:2]\n",
    "# batch_len = np.count_nonzero(batch_w, axis = 1)\n",
    "\n",
    "# NUM_SYMBOLIC_TAGS = 3\n",
    "# heads_pred, types_pred, _, _ = decode(batch_w, batch_c, leading_symbolic = NUM_SYMBOLIC_TAGS)\n",
    "# evaluate(batch_w, heads_pred, types_pred, batch_heads, batch_types, batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(words, heads_pred, types_pred, heads, types, lengths,\n",
    "             symbolic_root=False, symbolic_end=False):\n",
    "    batch_size, _ = words.shape\n",
    "    ucorr = 0.\n",
    "    lcorr = 0.\n",
    "    total = 0.\n",
    "    ucomplete_match = 0.\n",
    "    lcomplete_match = 0.\n",
    "\n",
    "    corr_root = 0.\n",
    "    total_root = 0.\n",
    "    start = 1 if symbolic_root else 0\n",
    "    end = 1 if symbolic_end else 0\n",
    "    for i in range(batch_size):\n",
    "        ucm = 1.\n",
    "        lcm = 1.\n",
    "        for j in range(start, lengths[i] - end):\n",
    "\n",
    "            total += 1\n",
    "            if heads[i, j] == heads_pred[i, j]:\n",
    "                ucorr += 1\n",
    "                if types[i, j] == types_pred[i, j]:\n",
    "                    lcorr += 1\n",
    "                else:\n",
    "                    lcm = 0\n",
    "            else:\n",
    "                ucm = 0\n",
    "                lcm = 0\n",
    "\n",
    "            if heads[i, j] == 0:\n",
    "                total_root += 1\n",
    "                corr_root += 1 if heads_pred[i, j] == 0 else 0\n",
    "\n",
    "        ucomplete_match += ucm\n",
    "        lcomplete_match += lcm\n",
    "\n",
    "    return (ucorr, lcorr, total, ucomplete_match, lcomplete_match), \\\n",
    "           (corr_root, total_root), batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "epoch = 20\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss, test_loss = [], []\n",
    "    pbar = tqdm(range(len(train)), desc = 'train minibatch loop')\n",
    "    for k in pbar:\n",
    "        batch_w, batch_c, batch_heads, batch_stacked_heads, batch_siblings, batch_children, \\\n",
    "        batch_stacked_types, batch_e, batch_d, batch_types, batch_len = train[k]\n",
    "        feed_dict = {model.words: batch_w,\n",
    "                     model.chars: batch_c,\n",
    "                     model.heads: batch_heads,\n",
    "                     model.stacked_heads: batch_stacked_heads,\n",
    "                     model.childrens: batch_children,\n",
    "                     model.siblings: batch_siblings,\n",
    "                     model.stacked_types: batch_stacked_types,\n",
    "                     model.mask_e: batch_e,\n",
    "                     model.mask_d: batch_d}\n",
    "        cost, _ = sess.run([model.cost, model.optimizer], feed_dict = feed_dict)\n",
    "        train_loss.append(cost)\n",
    "        pbar.set_postfix(cost = cost)\n",
    "    \n",
    "    pbar = tqdm(range(len(test)), desc = 'test minibatch loop')\n",
    "    for k in pbar:\n",
    "        batch_w, batch_c, batch_heads, batch_stacked_heads, batch_siblings, batch_children, \\\n",
    "        batch_stacked_types, batch_e, batch_d, batch_types, batch_len = test[k]\n",
    "        feed_dict = {model.words: batch_w,\n",
    "                     model.chars: batch_c,\n",
    "                     model.heads: batch_heads,\n",
    "                     model.stacked_heads: batch_stacked_heads,\n",
    "                     model.childrens: batch_children,\n",
    "                     model.siblings: batch_siblings,\n",
    "                     model.stacked_types: batch_stacked_types,\n",
    "                     model.mask_e: batch_e,\n",
    "                     model.mask_d: batch_d}\n",
    "        cost = sess.run(model.cost, feed_dict = feed_dict)\n",
    "        test_loss.append(cost)\n",
    "        pbar.set_postfix(cost = cost)\n",
    "    \n",
    "    print('epoch: %d, avg train loss: %f, avg test loss: %f'%(e + 1, np.mean(train_loss),\n",
    "                                                             np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
